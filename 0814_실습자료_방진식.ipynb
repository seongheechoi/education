{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m86_nxRAV3NH",
        "u5b6SAvu6E1q",
        "56-2ADbpJiH_",
        "s1jHha_wJl7g",
        "d-5dgufMQVoh",
        "uW1QxDzDJqlI",
        "OBsM-pS1aFWA",
        "5y-wGfLHaIur",
        "1TLeywDEaKhX",
        "_qnMpeUcaHC4",
        "4CsZ7HKswsVG",
        "vd7i1V_P76us",
        "tF1jix_w_Pyy",
        "h5l-TJAFH08L",
        "HRt9CpceHyRu"
      ],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongheechoi/education/blob/main/0814_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C_%EB%B0%A9%EC%A7%84%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef2mMFuh5j4R"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python:\", sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 세팅"
      ],
      "metadata": {
        "id": "S9uBGh6t6Kl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 환경 세팅: Gemini"
      ],
      "metadata": {
        "id": "Hu-YwiHcTuSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 무료 API\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\""
      ],
      "metadata": {
        "id": "qqxn5M0sDLxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "class Gemini:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.client = genai.Client()\n",
        "        self.generation_config = {\n",
        "            # 단어를 확률적으로 선택할 때 무작위성 정도를 제어 (0 ~ 1)\n",
        "            # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "            \"temperature\": 0.0,\n",
        "\n",
        "            # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "            \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "            \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "\n",
        "            # 랜덤 시드 값\n",
        "            \"seed\": 20250814,\n",
        "\n",
        "            # 최대 5개 설정 가능\n",
        "            \"stop_sequences\": [],\n",
        "\n",
        "            # 이전에 생성된 텍스트에 나타난 토큰이 다시 생성되는 것을 방지\n",
        "            # 0.0은 영향 없음\n",
        "            # 양수: 반복 줄이거나, 새로운 단어 더 사용\n",
        "            # 음수: 반복을 더 허용하거나, 기존 단어를 더 자주 사용\n",
        "            \"presence_penalty\": 0.0,\n",
        "            \"frequency_penalty\": 0.0\n",
        "        }\n",
        "\n",
        "    # 질문에 대한 토큰 개수를 카운트합니다.\n",
        "    def count_token(self, query, verbose=False):\n",
        "        response = self.client.models.count_tokens(\n",
        "            model=self.model_name,\n",
        "            contents=query,\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            # 추가 정보 제공\n",
        "            return response\n",
        "        else:\n",
        "            # 토큰 수만 제공\n",
        "            return response.total_tokens\n",
        "\n",
        "    # 질문에 대한 텍스트 응답을 생성하는 함수입니다.\n",
        "    def generate_text_response(self, query, streaming=False):\n",
        "        # 최종 결과 저장\n",
        "        total_response = \"\"\n",
        "\n",
        "        if streaming:\n",
        "            # 텍스트를 스트리밍으로 생성합니다.\n",
        "            response = self.client.models.generate_content_stream(\n",
        "                model=self.model_name,\n",
        "                contents=[query],\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=self.generation_config[\"temperature\"],\n",
        "                    top_p=self.generation_config[\"top_p\"],\n",
        "                    top_k=self.generation_config[\"top_k\"],\n",
        "                    candidate_count=1,\n",
        "                    seed=self.generation_config[\"seed\"],\n",
        "                    stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                    presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                    frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # 스트리밍 형식 출력\n",
        "            for chunk in response:\n",
        "                total_response += chunk.text\n",
        "                print(chunk.text, end=\"\")\n",
        "\n",
        "            return total_response\n",
        "\n",
        "        else:\n",
        "            # 텍스트를 생성하는 함수\n",
        "            response = self.client.models.generate_content(\n",
        "                model=self.model_name,\n",
        "                contents=query,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=self.generation_config[\"temperature\"],\n",
        "                    top_p=self.generation_config[\"top_p\"],\n",
        "                    top_k=self.generation_config[\"top_k\"],\n",
        "                    candidate_count=1,\n",
        "                    seed=self.generation_config[\"seed\"],\n",
        "                    stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                    presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                    frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                )\n",
        "            )\n",
        "            total_response = response.text\n",
        "            return total_response"
      ],
      "metadata": {
        "id": "Khh2R8YbDab-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image (4).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "qHsvcATVQONV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gemini-2.5-flash-lite\"\n",
        "gemini = Gemini(model_name)\n",
        "gemini.count_token(\"안녕하세요!\", verbose=True)"
      ],
      "metadata": {
        "id": "WKs1wpbjQEna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 환경 세팅: EXAONE with Huggingface"
      ],
      "metadata": {
        "id": "m86_nxRAV3NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, model_name, history_max_turns=10):\n",
        "        self.model_name = model_name\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=\"bfloat16\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.generation_config = {\n",
        "            # 단어를 확률적으로 선택할 때 무작위성 정도를 제어 (0 ~ 1)\n",
        "            # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "            \"temperature\": 0.1,\n",
        "\n",
        "            # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "            \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "            \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "\n",
        "            # 이전에 생성된 텍스트에 나타난 토큰이 다시 생성되는 것을 방지\n",
        "            # 0.0은 영향 없음\n",
        "            # 양수: 반복 줄이거나, 새로운 단어 더 사용\n",
        "            # 음수: 반복을 더 허용하거나, 기존 단어를 더 자주 사용\n",
        "            \"presence_penalty\": 1.5,\n",
        "            \"frequency_penalty\": 0.0\n",
        "        }\n",
        "\n",
        "    def generate_text_response(self, query, streaming=False, thinking=False):\n",
        "        # 스트리밍에 따라 다른 함수 선택\n",
        "        if streaming:\n",
        "            return self.generate_streaming(query, thinking)\n",
        "        else:\n",
        "            return self.generate_text(query, thinking)\n",
        "\n",
        "    def generate_text(self, query: str, thinking: bool = False):\n",
        "        # 입력 구성\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ]\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            enable_thinking=thinking,\n",
        "        )\n",
        "\n",
        "        # 출력 생성\n",
        "        output = self.model.generate(\n",
        "            input_ids.to(self.model.device),\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False if self.generation_config[\"temperature\"] == 0.0 else True,\n",
        "            temperature=self.generation_config[\"temperature\"],\n",
        "            top_p=self.generation_config[\"top_p\"],\n",
        "            top_k=self.generation_config[\"top_k\"],\n",
        "            repetition_penalty=1.5 if self.generation_config[\"presence_penalty\"] <= 0 else self.generation_config[\"presence_penalty\"],\n",
        "            diversity_penalty=self.generation_config[\"frequency_penalty\"]\n",
        "        )\n",
        "\n",
        "        # 텍스트 전처리\n",
        "        try:\n",
        "            response = self.tokenizer.decode(output[0]).replace(\"[|endofturn|]\", \"\")\n",
        "            think_stoken = response.find(\"<think>\")\n",
        "            think_etoken = response.find(\"</think>\")\n",
        "            reasoning = response[think_stoken + len(\"<think>\"):think_etoken]\n",
        "            answer = response[think_etoken + len(\"</think>\"):].strip()\n",
        "        except:\n",
        "            reasoning = \"\"\n",
        "            answer = response\n",
        "\n",
        "        # 반환\n",
        "        return reasoning, answer\n",
        "\n",
        "    def generate_streaming(self, query: str, thinking: bool = False):\n",
        "        # 입력 생성\n",
        "        messages = [{\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            enable_thinking=thinking,\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        # 스트리머 생성\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer,\n",
        "            skip_prompt=True,\n",
        "            skip_special_tokens=False if thinking else True,\n",
        "            decode_with_timestamps=False\n",
        "        )\n",
        "\n",
        "        # 별도 스레드에서 실행\n",
        "        gen_kwargs = dict(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False if self.generation_config[\"temperature\"] == 0.0 else True,\n",
        "            temperature=self.generation_config[\"temperature\"],\n",
        "            top_p=self.generation_config[\"top_p\"],\n",
        "            top_k=self.generation_config[\"top_k\"],\n",
        "            repetition_penalty=1.5 if self.generation_config[\"presence_penalty\"] <= 0 else self.generation_config[\"presence_penalty\"],\n",
        "            diversity_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "            streamer=streamer,\n",
        "        )\n",
        "        thread = Thread(target=self.model.generate, kwargs=gen_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        answer = []\n",
        "        for text in streamer:\n",
        "            answer.append(text)\n",
        "            print(text, end=\"\")\n",
        "\n",
        "        try:\n",
        "            reasoning, output = \"\".join(answer).split(\"</think>\")\n",
        "            return reasoning.strip(), output.replace(\"[|endofturn|]\", \"\").strip()\n",
        "        except:\n",
        "            return None, \"\".join(answer).replace(\"</think>\", \"\").replace(\"[|endofturn|]\", \"\").strip()"
      ],
      "metadata": {
        "id": "Iy5e-c-zWOP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
        "exaone = LLM(model_name)\n",
        "_, response = exaone.generate_text_response(\"안녕하세요?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_381zyuqfREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting"
      ],
      "metadata": {
        "id": "u5b6SAvu6E1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기본 입력"
      ],
      "metadata": {
        "id": "56-2ADbpJiH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 스트리밍 활성 함수\n",
        "streaming = True\n",
        "\n",
        "print(\"[입력]\")\n",
        "prompt = query = input()\n",
        "\n",
        "# _(언더바) 변수는 추후 Reasoning 파트에서 설명합니다.\n",
        "output = gemini.generate_text_response(prompt, streaming=streaming)\n",
        "# _, output = exaone.generate_text_response(prompt, streaming=streaming)\n",
        "\n",
        "if not streaming:\n",
        "    print()\n",
        "    print(\"[출력]\")\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "QX3yG4SsF8HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 하이퍼파라미터"
      ],
      "metadata": {
        "id": "s1jHha_wJl7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    # 단어를 확률적으로 선택할 때 무작위성 정도를 제어\n",
        "    # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "    # 클수록 단어들간의 선택 확률이 고르게 퍼져 다양한 응답이 생성 가능함 보통 0.4 ~ 0.7을 사용\n",
        "    \"temperature\": 0.5,\n",
        "\n",
        "    # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "    \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "    \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "\n",
        "    # 랜덤 시드 값\n",
        "    \"seed\": 20250814,\n",
        "\n",
        "    # 최대 5개 설정 가능\n",
        "    \"stop_sequences\": [],\n",
        "\n",
        "    # 이전에 생성된 텍스트에 나타난 토큰이 다시 생성되는 것을 방지\n",
        "    # 0.0은 영향 없음\n",
        "    # 양수: 반복 줄이거나, 새로운 단어 더 사용\n",
        "    # 음수:반복을 더 허용하거나, 기존 단어를 더 자주 사용\n",
        "    \"presence_penalty\": 0.0,\n",
        "    \"frequency_penalty\": 0.0\n",
        "}\n",
        "\n",
        "gemini.generation_config = generation_config.copy()\n",
        "exaone.generation_config = generation_config.copy()"
      ],
      "metadata": {
        "id": "1GKDVI66Pm6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streaming = True\n",
        "\n",
        "# 주석을 해제해서 원하는 걸 실습해보세요.\n",
        "# gemini.generation_config[\"temperature\"] = 0.0\n",
        "# gemini.generation_config[\"top_k\"] = 1\n",
        "# gemini.generation_config[\"top_p\"] = 0.95\n",
        "\n",
        "print(\"[입력]\")\n",
        "prompt = query = input()\n",
        "\n",
        "# _(언더바) 변수는 추후 Reasoning 파트에서 설명합니다.\n",
        "output = gemini.generate_text_response(prompt, streaming=streaming)\n",
        "# _, output = exaone.generate_text_response(prompt, streaming=streaming)\n",
        "\n",
        "if not streaming:\n",
        "    print()\n",
        "    print(\"[출력]\")\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "O8U-XUmnQKU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시스템 프롬프트"
      ],
      "metadata": {
        "id": "d-5dgufMQVoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"당신은 영어 선생님입니다. 사용자는 영어를 이제 입문한 수강생임을 고려하여 쉽고 친절하게 알려주되 3문장 이내로 짧고 명확하게 얘기하세요.\\n\\n\""
      ],
      "metadata": {
        "id": "lD0s4CniQXGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 스트리밍 활성화 변수\n",
        "streaming = True\n",
        "\n",
        "print(\"[입력]\")\n",
        "prompt = query = input()\n",
        "\n",
        "# _(언더바) 변수는 추후 Reasoning 파트에서 설명합니다.\n",
        "output = gemini.generate_text_response(system_prompt + prompt, streaming=streaming)\n",
        "# _, output = exaone.generate_text_response(system_prompt + prompt, streaming=streaming)\n",
        "\n",
        "if not streaming:\n",
        "    print()\n",
        "    print(\"[출력]\")\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "MPCZQpM6UJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-context Learning"
      ],
      "metadata": {
        "id": "uW1QxDzDJqlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Zero-shot"
      ],
      "metadata": {
        "id": "OBsM-pS1aFWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"\"\"\n",
        "질문: 눈은 어떻게 형성되나요?\n",
        "설명 1: 눈은 대기 중의 수증기가 얼어 얼음 결정이 되고, 이들이 서로 결합하며 성장해 대기를 떨어지면서 눈송이가 되며, 지면에 쌓이게 됩니다.\n",
        "설명 2: 수증기가 얼음 결정으로 얼어 눈이 형성됩니다.\n",
        "정답:\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(zero_shot_prompt, streaming=False)\n",
        "# _, output = exaone.generate_text_response(zero_shot_prompt, streaming=False)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "L_Y3mEXOevON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"유니스트\"\n",
        "zero_shot_prompt = f\"{len(word)}행시를 재밌게 생성해줘.\\n단어: {word}\"\n",
        "print([\"입력\"])\n",
        "print(prompt)\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(zero_shot_prompt, streaming=False)\n",
        "# _, output = exaone.generate_text_response(zero_shot_prompt, streaming=False)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "c2IlTyLgJqYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-shot"
      ],
      "metadata": {
        "id": "5y-wGfLHaIur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_shot = \"\"\"\n",
        "단어: 비행기\n",
        "비: 비가 오나 눈이 오나 바람이 부나 그리웠던 30년 세월\n",
        "행: 행여나 자식 잘 되길 바라시며 저 멀리 언덕 넘어 자식의 그림자를 바라보는 우리 어머니\n",
        "기: 기고만장\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "word = \"유니스트\"\n",
        "one_shot_prompt = f\"{len(word)}행시를 예제를 참고해서 재밌게 생성해줘.\\n\\n\" + one_shot + f\"\\n\\n단어: {word}\"\n",
        "print([\"입력\"])\n",
        "print(prompt)\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(one_shot_prompt, streaming=False)\n",
        "# _, output = exaone.generate_text_response(one_shot_prompt, streaming=False)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-TzkMTxvaR4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-shot"
      ],
      "metadata": {
        "id": "1TLeywDEaKhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"\n",
        "아래는 질문, 설명, 그리고 답변 형식을 보여주는 몇 가지 예시입니다:\n",
        "\n",
        "질문: 하늘이 왜 파란색인가요?\n",
        "설명1: 하늘이 파랗게 보이는 이유는 레일리 산란(Rayleigh scattering) 때문이며, 짧은 파장의 파란빛이 긴 파장의 붉은빛보다 더 쉽게 산란되어 하늘이 파랗게 보이게 됩니다.\n",
        "설명2: 레일리 산란 효과 때문입니다.\n",
        "답변: 설명2\n",
        "\n",
        "질문: 지진의 원인은 무엇인가요?\n",
        "설명1: 지각에서 갑작스럽게 에너지가 방출되는 현상.\n",
        "설명2: 지진은 판 구조가 갑자기 미끄러지거나 갈라지면서 에너지가 방출되고, 그로 인해 지진파가 발생하여 땅이 흔들리고 피해를 일으키는 현상입니다.\n",
        "답변: 설명1\n",
        "\n",
        "이제, 위의 예시 형식을 참고하여 다음 질문에 답하세요:\n",
        "\n",
        "질문: 눈은 어떻게 형성되나요?\n",
        "설명1: 공기 중의 수증기가 대기에서 얼음 결정으로 얼고, 이것이 서로 합쳐져 눈송이로 성장하면서 대기를 통과해 내려오고 지면에 쌓이게 됩니다.\n",
        "설명2: 수증기가 얼음 결정으로 얼어 눈이 됩니다.\n",
        "답변:\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(few_shot_prompt, streaming=False)\n",
        "# _, output = exaone.generate_text_response(few_shot_prompt, streaming=False)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "Ua3X-afofLEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot = \"\"\"\n",
        "단어: 비행기\n",
        "비: 비가 오나 눈이 오나 바람이 부나 그리웠던 30년 세월\n",
        "행: 행여나 자식 잘 되길 바라시며 저 멀리 언덕 넘어 자식의 그림자를 바라보는 우리 어머니\n",
        "기: 기고만장\n",
        "\n",
        "단어: 아버지\n",
        "아: 아버지가 굴을 따오셨네\n",
        "버: 버선발로 굴을 따고 계시네\n",
        "지: 지가 다 먹네\n",
        "\n",
        "단어: 코딱지\n",
        "코: 코브라를 잡기 위해서는\n",
        "딱: 딱 전기로 잡아야죠\n",
        "지: 지직 지직 지지직\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "word = \"유니스트\"\n",
        "few_shot_prompt = f\"{len(word)}행시를 예제를 참고해서 재밌게 생성해줘.\\n\\n\" + few_shot + f\"\\n\\n단어: {word}\"\n",
        "# print([\"입력\"])\n",
        "# print(prompt)\n",
        "# print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(few_shot_prompt, streaming=False)\n",
        "# _, output = exaone.generate_text_response(few_shot_prompt, streaming=False)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "i3Pj27J6aIA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain-of-Thought"
      ],
      "metadata": {
        "id": "_qnMpeUcaHC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"놀이터에서 아이들이 모두 반팔 옷을 입고 있다면 날씨는 어떨까?\"\n",
        "print(\"[입력]\")\n",
        "print(prompt)\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(prompt, streaming=True)\n",
        "# _, output = exaone.generate_text_response(prompt, streaming=True)"
      ],
      "metadata": {
        "id": "k6TjhIMXrnPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CoT_prompt = \"\"\"\n",
        "질문:\n",
        "- 지하철역 앞에서 많은 사람들이 우산을 쓰고 있다면 하늘 상태는 어떨까?\n",
        "\n",
        "추론:\n",
        "- 많은 사람들이 동시에 우산을 쓰는 경우는 보통 비가 올 때다.\n",
        "- 현재 많은 사람들이 우산을 쓰고 있다.\n",
        "- 따라서 비가 오고 있을 가능성이 높다.\n",
        "\n",
        "결론:\n",
        "- 비가 오고 있다.\n",
        "\n",
        "새로운 질문:\n",
        "놀이터에서 아이들이 모두 반팔 옷을 입고 있다면 날씨는 어떨까?\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "# print(\"[입력]\")\n",
        "# print(CoT_prompt)\n",
        "# print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(CoT_prompt, streaming=True)\n",
        "# _, output = exaone.generate_text_response(CoT_prompt, streaming=True)"
      ],
      "metadata": {
        "id": "Z7BP-0VtrXbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CsZ7HKswsVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gemini\n",
        "\n",
        "> 최근 모델들은 특별한 명시를 하지 않아도 단계별 이유를 추론하기도 합니다.\n"
      ],
      "metadata": {
        "id": "vd7i1V_P76us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"숫자만 입력하세요. 상자에 파란 공이 7개, 빨간 공이 3개 합쳐서 10개가 있다. 복원하지 않고 연속으로 7개를 뽑을 때, 파란 공이 5개, 빨간 공이 2개를 추출할 확률은?\"\n",
        "print(\"[입력]\")\n",
        "print(prompt)\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(prompt, streaming=True)"
      ],
      "metadata": {
        "id": "x15Oaagjoak9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"상자에 파란 공이 14개, 빨간 공이 6개 합쳐서 20개가 있다. 복원하지 않고 연속으로 7개를 뽑을 때, 파란 공이 5개, 빨간 공이 2개를 추출할 확률은?\"\n",
        "print(\"[입력]\")\n",
        "print(prompt)\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "output = gemini.generate_text_response(prompt, streaming=True)"
      ],
      "metadata": {
        "id": "f7wfSqh46EVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXAONE\n",
        "\n",
        "> EXAONE 모델은 생각을 할 수 있게, 하지 않게 조절이 가능합니다."
      ],
      "metadata": {
        "id": "jrKY6k_r7__a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[입력]\")\n",
        "prompt = input()\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "reasoning, output = exaone.generate_text_response(prompt, streaming=True)"
      ],
      "metadata": {
        "id": "CRjhaWe578rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[입력]\")\n",
        "prompt = input()\n",
        "print()\n",
        "\n",
        "print(\"[출력]\")\n",
        "reasoning, output = exaone.generate_text_response(prompt, streaming=True, thinking=True)"
      ],
      "metadata": {
        "id": "ht-yVg2777-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversations"
      ],
      "metadata": {
        "id": "tF1jix_w_Pyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini-Chat"
      ],
      "metadata": {
        "id": "h5l-TJAFH08L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "class Gemini:\n",
        "    def __init__(self, model_name, system_prompt=None):\n",
        "        self.model_name = model_name\n",
        "        self.client = genai.Client()\n",
        "        self.generation_config = {\n",
        "            # 단어를 확률적으로 선택할 때 무작위성 정도를 제어 (0 ~ 1)\n",
        "            # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "            \"temperature\": 0.0,\n",
        "\n",
        "            # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "            \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "            \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "\n",
        "            # 랜덤 시드 값\n",
        "            \"seed\": 20250814,\n",
        "\n",
        "            # 최대 5개 설정 가능\n",
        "            \"stop_sequences\": [],\n",
        "\n",
        "            # 이전에 생성된 텍스트에 나타난 토큰이 다시 생성되는 것을 방지\n",
        "            # 0.0은 영향 없음\n",
        "            # 양수: 반복 줄이거나, 새로운 단어 더 사용\n",
        "            # 음수: 반복을 더 허용하거나, 기존 단어를 더 자주 사용\n",
        "            \"presence_penalty\": 0.0,\n",
        "            \"frequency_penalty\": 0.0\n",
        "        }\n",
        "\n",
        "        self.system_prompt = system_prompt\n",
        "        self.chat = self.client.chats.create(model=self.model_name)\n",
        "\n",
        "    # 질문에 대한 토큰 개수를 카운트합니다.\n",
        "    def count_token(self, query, verbose=False):\n",
        "        response = self.client.models.count_tokens(\n",
        "            model=self.model_name,\n",
        "            contents=query,\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            # 추가 정보 제공\n",
        "            return response\n",
        "        else:\n",
        "            # 토큰 수만 제공\n",
        "            return response.total_tokens\n",
        "\n",
        "    def reset_history(self, system_prompt):\n",
        "        del self.chat\n",
        "        self.chat = self.client.chats.create(model=self.model_name)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def set_system_prompt(self, system_prompt):\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def get_history(self):\n",
        "        for message in self.chat.get_history():\n",
        "            print(f'role - {message.role}',end=\": \")\n",
        "            print(message.parts[0].text)\n",
        "\n",
        "    # 질문에 대한 텍스트 응답을 생성하는 함수입니다.\n",
        "    def generate_text_response(self, query, streaming=False, conversation=False):\n",
        "        # 최종 결과 저장\n",
        "        total_response = \"\"\n",
        "\n",
        "        # 채팅 (멀티 턴)\n",
        "        if conversation:\n",
        "            if streaming:\n",
        "                # 텍스트를 스트리밍으로 생성합니다.\n",
        "                response = self.chat.send_message_stream(\n",
        "                    query,\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        system_instruction=self.system_prompt,\n",
        "                        temperature=self.generation_config[\"temperature\"],\n",
        "                        top_p=self.generation_config[\"top_p\"],\n",
        "                        top_k=self.generation_config[\"top_k\"],\n",
        "                        candidate_count=1,\n",
        "                        seed=self.generation_config[\"seed\"],\n",
        "                        stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                        presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                        frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # 스트리밍 형식 출력\n",
        "                for chunk in response:\n",
        "                    total_response += chunk.text\n",
        "                    print(chunk.text, end=\"\")\n",
        "\n",
        "                return total_response\n",
        "            else:\n",
        "                response = self.chat.send_message(\n",
        "                    query,\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        system_instruction=self.system_prompt,\n",
        "                        temperature=self.generation_config[\"temperature\"],\n",
        "                        top_p=self.generation_config[\"top_p\"],\n",
        "                        top_k=self.generation_config[\"top_k\"],\n",
        "                        candidate_count=1,\n",
        "                        seed=self.generation_config[\"seed\"],\n",
        "                        stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                        presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                        frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                    )\n",
        "                )\n",
        "                total_response = response.text\n",
        "                return total_response\n",
        "        # 단일 턴\n",
        "        else:\n",
        "            if streaming:\n",
        "                # 텍스트를 스트리밍으로 생성합니다.\n",
        "                response = self.client.models.generate_content_stream(\n",
        "                    model=self.model_name,\n",
        "                    contents=[query],\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        temperature=self.generation_config[\"temperature\"],\n",
        "                        top_p=self.generation_config[\"top_p\"],\n",
        "                        top_k=self.generation_config[\"top_k\"],\n",
        "                        candidate_count=1,\n",
        "                        seed=self.generation_config[\"seed\"],\n",
        "                        stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                        presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                        frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # 스트리밍 형식 출력\n",
        "                for chunk in response:\n",
        "                    total_response += chunk.text\n",
        "                    print(chunk.text, end=\"\")\n",
        "\n",
        "                return total_response\n",
        "\n",
        "            else:\n",
        "                # 텍스트를 생성하는 함수\n",
        "                response = self.client.models.generate_content(\n",
        "                    model=self.model_name,\n",
        "                    contents=query,\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        temperature=self.generation_config[\"temperature\"],\n",
        "                        top_p=self.generation_config[\"top_p\"],\n",
        "                        top_k=self.generation_config[\"top_k\"],\n",
        "                        candidate_count=1,\n",
        "                        seed=self.generation_config[\"seed\"],\n",
        "                        stop_sequences=self.generation_config[\"stop_sequences\"],\n",
        "                        presence_penalty=self.generation_config[\"presence_penalty\"],\n",
        "                        frequency_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "                    )\n",
        "                )\n",
        "                total_response = response.text\n",
        "                return total_response"
      ],
      "metadata": {
        "id": "yvCmf8zUBFWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gemini-2.5-flash-lite\"\n",
        "gemini = Gemini(model_name)\n",
        "gemini.count_token(\"안녕하세요!\", verbose=True)"
      ],
      "metadata": {
        "id": "zXYHM45YH36B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXAONE-Chat"
      ],
      "metadata": {
        "id": "HRt9CpceHyRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, model_name, system_prompt=None, history_max_turns=10):\n",
        "        self.model_name = model_name\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=\"bfloat16\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.generation_config = {\n",
        "            # 단어를 확률적으로 선택할 때 무작위성 정도를 제어 (0 ~ 1)\n",
        "            # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "            \"temperature\": 0.1,\n",
        "\n",
        "            # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "            \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "            \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "\n",
        "            # 이전에 생성된 텍스트에 나타난 토큰이 다시 생성되는 것을 방지\n",
        "            # 0.0은 영향 없음\n",
        "            # 양수: 반복 줄이거나, 새로운 단어 더 사용\n",
        "            # 음수: 반복을 더 허용하거나, 기존 단어를 더 자주 사용\n",
        "            \"presence_penalty\": 1.5,\n",
        "            \"frequency_penalty\": 0.0\n",
        "        }\n",
        "\n",
        "        # 대화 히스토리 (chat template와 호환되는 포맷)\n",
        "        self.history = []\n",
        "        if system_prompt:\n",
        "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "        self.history_max_turns = int(history_max_turns)\n",
        "\n",
        "    # ----------- 히스토리 및 시스템 프롬프트 설정 관련 -----------\n",
        "    def reset_history(self, system_prompt=None):\n",
        "        self.history = []\n",
        "        if system_prompt:\n",
        "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    def set_system_prompt(self, system_prompt):\n",
        "        # 기존 system을 교체\n",
        "        self.history = [m for m in self.history if m.get(\"role\") != \"system\"]\n",
        "        if system_prompt:\n",
        "            self.history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    def get_history(self):\n",
        "        return list(self.history)\n",
        "\n",
        "    def load_history(self, messages):\n",
        "        # messages는 [{\"role\": \"...\", \"content\": \"...\"}] 리스트\n",
        "        self.history = list(messages)\n",
        "\n",
        "     # 메세지 빌드 관련 함수\n",
        "    def _build_messages(self, query, conversation):\n",
        "        if conversation and self.history:\n",
        "            return self.history + [{\"role\": \"user\", \"content\": query}]\n",
        "        else:\n",
        "            return [{\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    # 히스토리 업데이트 함수\n",
        "    def _truncate_history_inplace(self):\n",
        "        # history_max_turns: user/assistant 쌍을 입력으로 기준을 두며 system은 항상 보존합니다.\n",
        "        if not self.history: return\n",
        "        system_msgs = [m for m in self.history if m[\"role\"] == \"system\"]\n",
        "        non_system = [m for m in self.history if m[\"role\"] != \"system\"]\n",
        "\n",
        "        # user/assistant를 뒤에서부터 최대 (history_max_turns * 2)개 유지\n",
        "        keep = self.history_max_turns * 2\n",
        "        if len(non_system) > keep:\n",
        "            non_system = non_system[-keep:]\n",
        "\n",
        "        self.history = system_msgs + non_system\n",
        "\n",
        "    def generate_text_response(self, query, streaming=False, thinking=False, conversation=False):\n",
        "        # 스트리밍에 따라 다른 함수 선택\n",
        "        if streaming:\n",
        "            return self.generate_streaming(query, thinking, conversation)\n",
        "        else:\n",
        "            return self.generate_text(query, thinking, conversation)\n",
        "\n",
        "    def generate_text(self, query: str, thinking: bool = False, conversation: bool = False):\n",
        "        # 입력 구성\n",
        "        messages = self._build_messages(query, conversation=conversation)\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            enable_thinking=thinking,\n",
        "        )\n",
        "\n",
        "        # 출력 생성\n",
        "        output = self.model.generate(\n",
        "            input_ids.to(self.model.device),\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False if self.generation_config[\"temperature\"] == 0.0 else True,\n",
        "            temperature=self.generation_config[\"temperature\"],\n",
        "            top_p=self.generation_config[\"top_p\"],\n",
        "            top_k=self.generation_config[\"top_k\"],\n",
        "            repetition_penalty=1.5 if self.generation_config[\"presence_penalty\"] <= 0 else self.generation_config[\"presence_penalty\"],\n",
        "            diversity_penalty=self.generation_config[\"frequency_penalty\"]\n",
        "        )\n",
        "\n",
        "        # 텍스트 전처리\n",
        "        try:\n",
        "            response = self.tokenizer.decode(output[0]).replace(\"[|endofturn|]\", \"\")\n",
        "            think_stoken = response.rfind(\"<think>\")\n",
        "            think_etoken = response.rfind(\"</think>\")\n",
        "            reasoning = response[think_stoken + len(\"<think>\"):think_etoken]\n",
        "            answer = response[think_etoken + len(\"</think>\"):].strip()\n",
        "        except:\n",
        "            reasoning = \"\"\n",
        "            answer = response\n",
        "\n",
        "        # 대화 히스토리에 저장\n",
        "        if conversation:\n",
        "            # thinking=True이면 모델이 출력한 <think> 블록을 assistant에 함께 보관할지 선택\n",
        "            assistant_content = (f\"<think>{reasoning}</think>\\n{answer}\" if thinking and reasoning else answer)\n",
        "\n",
        "            self.history.append({\"role\": \"user\", \"content\": query})\n",
        "            self.history.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
        "            self._truncate_history_inplace()\n",
        "\n",
        "        # 반환\n",
        "        return reasoning, answer\n",
        "\n",
        "    def generate_streaming(self, query: str, thinking: bool = False, conversation: bool = False):\n",
        "        # 입력 생성\n",
        "        messages = self._build_messages(query, conversation=conversation)\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            enable_thinking=thinking,\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        # 스트리머 생성\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer,\n",
        "            skip_prompt=True,\n",
        "            skip_special_tokens=False if thinking else True,\n",
        "            decode_with_timestamps=False\n",
        "        )\n",
        "\n",
        "        # 별도 스레드에서 실행\n",
        "        gen_kwargs = dict(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False if self.generation_config[\"temperature\"] == 0.0 else True,\n",
        "            temperature=self.generation_config[\"temperature\"],\n",
        "            top_p=self.generation_config[\"top_p\"],\n",
        "            top_k=self.generation_config[\"top_k\"],\n",
        "            repetition_penalty=1.5 if self.generation_config[\"presence_penalty\"] <= 0 else self.generation_config[\"presence_penalty\"],\n",
        "            diversity_penalty=self.generation_config[\"frequency_penalty\"],\n",
        "            streamer=streamer,\n",
        "        )\n",
        "        thread = Thread(target=self.model.generate, kwargs=gen_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        answer = []\n",
        "        for text in streamer:\n",
        "            answer.append(text)\n",
        "            print(text, end=\"\")\n",
        "\n",
        "        try:\n",
        "            reasoning, output = \"\".join(answer).split(\"</think>\")\n",
        "            reasoning = reasoning.strip()\n",
        "            output = output.replace(\"[|endofturn|]\", \"\").strip()\n",
        "        except:\n",
        "            reasoning = \"\"\n",
        "            output = \"\".join(answer).replace(\"</think>\", \"\").replace(\"[|endofturn|]\", \"\").strip()\n",
        "\n",
        "        if conversation:\n",
        "            assistant_content = (f\"<think>{reasoning}</think>\\n{output}\" if thinking and reasoning else output)\n",
        "\n",
        "            self.history.append({\"role\": \"user\", \"content\": query})\n",
        "            self.history.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
        "            self._truncate_history_inplace()\n",
        "\n",
        "        return reasoning, output"
      ],
      "metadata": {
        "id": "MOYp1-GEBCcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
        "exaone = LLM(model_name)\n",
        "_, response = exaone.generate_text_response(\"안녕하세요?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "gQhGRiIACvvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자유롭게 채팅 실습하시면 됩니다."
      ],
      "metadata": {
        "id": "K0WCY_ulFkHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 아래 주석을 풀어 초기화하고 시스템 프롬프트를 초기화할 수 있습니다.\n",
        "# system_prompt = \"너는 영어 교사야.\"\n",
        "# gemini.reset_history(system_prompt)\n",
        "# exaone.reset_history(system_prompt)\n",
        "\n",
        "## 아래 주석을 풀어 대화 도중에 시스템 프롬프트를 변경할 수 있습니다.\n",
        "# system_prompt = \"나는 영어보단 한국어를 잘하는 교사야.\"\n",
        "# gemini.system_prompt = system_prompt\n",
        "# exaone.set_system_prompt(system_prompt)\n",
        "\n",
        "print(\"[입력]\")\n",
        "prompt = input()\n",
        "\n",
        "print(\"\\n[출력]\")\n",
        "response = gemini.generate_text_response(prompt, conversation=True, streaming=True)\n",
        "# _, response = exaone.generate_text_response(prompt, conversation=True, streaming=True)"
      ],
      "metadata": {
        "id": "5OZSfIWRFNKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토리 확인\n",
        "gemini.get_history()"
      ],
      "metadata": {
        "id": "4paz7ML3IMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토리 확인\n",
        "exaone.history"
      ],
      "metadata": {
        "id": "upVeU5ZIIJQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "yiIjI0WEL5m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet langchain-core==0.1.23\n",
        "%pip install --quiet langchain==0.1.1\n",
        "%pip install --quiet langchain-google-genai==0.0.6\n",
        "%pip install --quiet -U langchain-community==0.0.20"
      ],
      "metadata": {
        "id": "00Pas_sxL725"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LangChain에서도 generation_config를 수정하여 애플리케이션 적용이 가능합니다.\n",
        "generation_config = {\n",
        "    # 단어를 확률적으로 선택할 때 무작위성 정도를 제어 (0 ~ 1)\n",
        "    # 0.0은 가장 높은 확률을 가진 단어만을 선택함\n",
        "    \"temperature\": 0.0,\n",
        "\n",
        "    # 모델이 출력할 토큰을 선택하는 방식을 변경\n",
        "    \"top_p\": 0.95, # 0 ~ 1, 누적 확률내에서 확률 기반 추출\n",
        "    \"top_k\": 20,   # 최대 k 단어 내에서 확률 기반 추출\n",
        "}\n",
        "\n",
        "model_name = \"gemini-2.5-flash-lite\"\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=model_name,\n",
        "    generation_config = generation_config\n",
        ")"
      ],
      "metadata": {
        "id": "td7bIw-wPA_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대용량 문서 요약용 라이브러리\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema.prompt_template import format_document\n",
        "\n",
        "# 입력 및 출력 과정 중 사용되는 템플릿, Parser (구조화된 형태로 변환해주는 도구)\n",
        "from langchain import PromptTemplate\n",
        "from langchain.schema import StrOutputParser"
      ],
      "metadata": {
        "id": "Fm6IS1jrOnz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL은 자유롭게 기입해서 실습해도 괜찮습니다.\n",
        "URL = \"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\"\n",
        "loader = WebBaseLoader(URL) # HTML을 문서로 변환\n",
        "docs = loader.load()        # 여러 문서들로 분할됨\n",
        "\n",
        "# 문서의 키\n",
        "print(docs[0].__dict__.keys())"
      ],
      "metadata": {
        "id": "erQGxhCVOxV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 내용 추출을 위한 프롬프트 템플릿\n",
        "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
        "\n",
        "# Gemini에 질문하기 위한 프롬프트 템플릿\n",
        "llm_prompt_template = \"\"\"\n",
        "다음 문서에서 간결한 요약본을 작성하세요:\n",
        "\"{text}\"\n",
        "간결한 요약본:\n",
        "\"\"\"[1:-1]\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(\n",
        "    template=llm_prompt_template\n",
        ")\n",
        "print(llm_prompt)"
      ],
      "metadata": {
        "id": "I94zTpu4PvM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LCEL (LangChain Expression Language)\n",
        "# LangChain 파이프라인 생성\n",
        "stuff_chain = (\n",
        "    {\n",
        "        \"text\": lambda docs: \"\\n\\n\".join(\n",
        "            format_document(doc, doc_prompt) for doc in docs\n",
        "        )\n",
        "    }                    # doc을 doc_prompt에 넣어 page_content 값만을 채택\n",
        "    | llm_prompt         # Gemini를 위한 프롬프트\n",
        "    | llm                # Gemini 응답\n",
        "    | StrOutputParser()  # 결과 반환 파싱\n",
        ")"
      ],
      "metadata": {
        "id": "KNoYsp3JP5xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과\n",
        "output = stuff_chain.invoke(docs)"
      ],
      "metadata": {
        "id": "p-iBuEQAXYxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_batch = stuff_chain.batch([docs, docs])"
      ],
      "metadata": {
        "id": "iQZiqqanYAeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream = stuff_chain.stream(docs)\n",
        "for text in stream:\n",
        "    print(text, end=\" \")"
      ],
      "metadata": {
        "id": "m1AN4hQYXc60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "O-wPc1HBYkKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 만화 생성"
      ],
      "metadata": {
        "id": "aoohCUu7irWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain-core>=0.2.10,<0.3.0\"\n",
        "%pip install \"langchain>=0.2.10,<0.3.0\"\n",
        "%pip install \"langchain-community>=0.2.10,<0.3.0\"\n",
        "%pip install \"langchain-google-genai>=1.0.0\"\n",
        "%pip install \"langgraph\"\n",
        "%pip install google-genai pillow"
      ],
      "metadata": {
        "id": "X-uzKNp7anJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import base64, pathlib, os\n",
        "\n",
        "import os, math\n",
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "id": "bK5v35zGaBbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 무료 API\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\"\n",
        "\n",
        "model_name = \"gemini-2.5-flash-lite\"\n",
        "llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.7)\n",
        "\n",
        "generator_name = \"gemini-2.0-flash-preview-image-generation\"\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "Ar7i084gbigw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------\n",
        "# 1) 데이터 스키마\n",
        "# ---------------------------------------------------\n",
        "class Character(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "    age: Optional[str] = \"\"\n",
        "    appearance: str\n",
        "    personality: str\n",
        "    color_palette: List[str] = Field(default_factory=list)\n",
        "    visual_tags: List[str] = Field(default_factory=list)\n",
        "\n",
        "class WorldSetting(BaseModel):\n",
        "    locations: List[str] = Field(default_factory=list)\n",
        "    era: Optional[str] = \"\"\n",
        "    style_ref: Optional[str] = \"\"\n",
        "\n",
        "class StyleGuide(BaseModel):\n",
        "    drawing_style: str\n",
        "    color_style: str\n",
        "    do_not_use: List[str] = Field(default_factory=list)\n",
        "    aspect_ratio: str = \"square\"\n",
        "\n",
        "class StoryFrame(BaseModel):\n",
        "    index: int\n",
        "    summary: str\n",
        "    key_events: List[str] = Field(default_factory=list)\n",
        "    characters: List[str] = Field(default_factory=list)\n",
        "    emotion: str\n",
        "    camera: str\n",
        "    lighting: str\n",
        "    palette: List[str] = Field(default_factory=list)\n",
        "    speech_bubbles: List[str] = Field(default_factory=list)\n",
        "\n",
        "class ImagePrompt(BaseModel):\n",
        "    index: int\n",
        "    prompt: str\n",
        "    negative_prompt: str\n",
        "    seed: Optional[int] = None\n",
        "    reference_images: List[str] = Field(default_factory=list)\n",
        "    pose_hint: Optional[str] = \"\"\n",
        "    width: int = 1024\n",
        "    height: int = 1024\n",
        "\n",
        "class StoryPlan(BaseModel):\n",
        "    title: str\n",
        "    synopsis: str\n",
        "    characters: List[Character]\n",
        "    world: WorldSetting\n",
        "    style: StyleGuide\n",
        "    frames: List[StoryFrame] = Field(default_factory=list)\n",
        "\n",
        "class PipelineOutput(BaseModel):\n",
        "    story: StoryPlan\n",
        "    image_prompts: List[ImagePrompt]"
      ],
      "metadata": {
        "id": "FwuQtRgPaZVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------\n",
        "# 2) 프롬프트 템플릿들\n",
        "# ---------------------------------------------------\n",
        "story_and_bible_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"너는 만화 기획 작가다. 사용자의 입력을 바탕으로 장편 스토리를 쓰고, \"\n",
        "     \"캐릭터/세계관/스타일 가이드를 구조화해서 일관성을 보장한다. \"\n",
        "     \"반환은 JSON이며 반드시 스키마를 따른다.\"),\n",
        "    (\"human\",\n",
        "     \"입력: {user_input}\\n\"\n",
        "     \"키워드: {keywords}\\n\"\n",
        "     \"장르: {genre}\\n\"\n",
        "     \"대상독자: {audience}\\n\"\n",
        "     \"톤: {tone}\\n\"\n",
        "     \"요청 분량(문단 수 기준): {paragraphs}\\n\\n\"\n",
        "     \"아래 JSON 스키마로 응답:\\n\"\n",
        "     \"{schema}\")\n",
        "])\n",
        "\n",
        "scene_split_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"너는 시나리오 편집자다. 주어진 줄거리를 n개의 만화 장면으로 나눈다. \"\n",
        "     \"각 장면은 한 컷에 담길 정도로 간결해야 한다. \"\n",
        "     \"장면 간 서사 흐름/인물/톱니감정의 일관성을 유지해라. \"\n",
        "     \"반환은 frames 배열(JSON).\"),\n",
        "    (\"human\",\n",
        "     \"줄거리:\\n{synopsis}\\n\\n\"\n",
        "     \"n: {n}\\n\"\n",
        "     \"캐릭터 목록: {character_names}\\n\"\n",
        "     \"스타일 가이드: {style_guide}\\n\"\n",
        "     \"아래 스키마로 frames를 반환:\\n{schema}\")\n",
        "])\n",
        "\n",
        "prompt_gen_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"너는 이미지 프롬프트 엔지니어다. 각 장면에 대해 이미지 생성 모델용 프롬프트를 만든다. \"\n",
        "     \"모든 프롬프트에 캐릭터 바이블과 스타일 가이드를 반영하고 시각적 일관성을 유지해라. \"\n",
        "     \"말풍선 텍스트가 있으면 배치 힌트를 포함해라(예: '좌상단 작은 말풍선'). \"\n",
        "     \"가능하면 카메라/조명/팔레트/구도 키워드를 풍부하게 사용하되 불필요한 수식은 피한다. \"\n",
        "     \"금지 요소는 negative prompt에 넣는다. \"\n",
        "     \"반환은 ImagePrompt의 리스트(JSON).\"),\n",
        "    (\"human\",\n",
        "     \"입력:\\n\"\n",
        "     \"캐릭터 바이블: {character_bible}\\n\"\n",
        "     \"세계관: {world}\\n\"\n",
        "     \"스타일 가이드: {style}\\n\"\n",
        "     \"frames: {frames}\\n\"\n",
        "     \"기본 해상도: {width}x{height}\\n\"\n",
        "     \"시드: {seed}\\n\"\n",
        "     \"참조 이미지: {references}\\n\\n\"\n",
        "     \"아래 스키마로 응답:\\n{schema}\")\n",
        "])"
      ],
      "metadata": {
        "id": "wc2K5jAEcAwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------\n",
        "# 3) 체인 빌더들\n",
        "# ---------------------------------------------------\n",
        "def build_story_and_bible_chain():\n",
        "    parser = JsonOutputParser(pydantic_object=StoryPlan)\n",
        "    return story_and_bible_template | llm | parser\n",
        "\n",
        "def build_scene_split_chain():\n",
        "    # 실제론 dict 리스트가 올 수 있어 안전 캐스팅 필요\n",
        "    parser = JsonOutputParser(pydantic_object=List[StoryFrame])\n",
        "    return scene_split_template | llm | parser\n",
        "\n",
        "def build_prompt_chain():\n",
        "    # 실제론 dict 리스트가 올 수 있어 안전 캐스팅 필요\n",
        "    parser = JsonOutputParser(pydantic_object=List[ImagePrompt])\n",
        "    return prompt_gen_template | llm | parser\n",
        "\n",
        "story_chain = build_story_and_bible_chain()\n",
        "split_chain = build_scene_split_chain()\n",
        "prompt_chain = build_prompt_chain()"
      ],
      "metadata": {
        "id": "rZ5tmErtcDbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------\n",
        "# 4) LangGraph 상태 & 노드\n",
        "# ---------------------------------------------------\n",
        "class GraphState(BaseModel):\n",
        "    user_input: str\n",
        "    keywords: List[str] = Field(default_factory=list)\n",
        "    n: int\n",
        "    genre: str = \"adventure\"\n",
        "    audience: str = \"teen\"\n",
        "    tone: str = \"warm and witty\"\n",
        "    paragraphs: int = 8\n",
        "    width: int = 1024\n",
        "    height: int = 1024\n",
        "    seed: Optional[int] = 1234\n",
        "    references: Dict[str, List[str]] = Field(default_factory=dict)\n",
        "    story_plan: Optional[StoryPlan] = None\n",
        "    image_prompts: Optional[List[ImagePrompt]] = None\n",
        "    outputs_dir: str = \"outputs\"\n",
        "    generated_images: Dict[int, str] = Field(default_factory=dict)\n",
        "    generation_texts: Dict[int, str] = Field(default_factory=dict)\n",
        "\n",
        "def node_story_and_bible(state: GraphState):\n",
        "    schema = StoryPlan.schema_json(indent=2, sort_keys=True)\n",
        "    result = story_chain.invoke({\n",
        "        \"user_input\": state.user_input,\n",
        "        \"keywords\": \", \".join(state.keywords) if state.keywords else \"없음\",\n",
        "        \"genre\": state.genre,\n",
        "        \"audience\": state.audience,\n",
        "        \"tone\": state.tone,\n",
        "        \"paragraphs\": state.paragraphs,\n",
        "        \"schema\": schema\n",
        "    })\n",
        "\n",
        "    # 혹시 dict로 왔을 때 안전 캐스팅\n",
        "    if isinstance(result, dict):\n",
        "        result = StoryPlan(**result)\n",
        "\n",
        "    # 내부 필드도 혹시 dict일 수 있으니 한 번 더 보정\n",
        "    result.world = WorldSetting(**result.world) if isinstance(result.world, dict) else result.world\n",
        "    result.style = StyleGuide(**result.style) if isinstance(result.style, dict) else result.style\n",
        "    result.characters = [Character(**c) if isinstance(c, dict) else c for c in result.characters]\n",
        "    state.story_plan = result\n",
        "    return state\n",
        "\n",
        "def node_scene_split(state: GraphState):\n",
        "    schema = StoryFrame.schema_json(indent=2, sort_keys=True)\n",
        "    style_guide_short = f\"{state.story_plan.style.drawing_style}; {state.story_plan.style.color_style}\"\n",
        "    character_names = [c.name for c in state.story_plan.characters]\n",
        "\n",
        "    frames_raw = split_chain.invoke({\n",
        "        \"synopsis\": state.story_plan.synopsis,\n",
        "        \"n\": state.n,\n",
        "        \"character_names\": \", \".join(character_names),\n",
        "        \"style_guide\": style_guide_short,\n",
        "        \"schema\": schema\n",
        "    })\n",
        "\n",
        "    # 혹시 {\"frames\":[...]} 형태로 줄 때 흡수\n",
        "    if isinstance(frames_raw, dict) and \"frames\" in frames_raw:\n",
        "        frames_iter = frames_raw[\"frames\"]\n",
        "    else:\n",
        "        frames_iter = frames_raw\n",
        "\n",
        "    # 여기서 전부 StoryFrame으로 강제 변환 + index 보정\n",
        "    frames: List[StoryFrame] = [coerce_storyframe(item, i) for i, item in enumerate(frames_iter)]\n",
        "\n",
        "    state.story_plan.frames = frames\n",
        "    return state\n",
        "\n",
        "\n",
        "def node_prompt_gen(state: GraphState):\n",
        "    schema = ImagePrompt.schema_json(indent=2, sort_keys=True)\n",
        "    character_bible = [\n",
        "        {\n",
        "            \"name\": c.name, \"appearance\": c.appearance, \"personality\": c.personality,\n",
        "            \"visual_tags\": c.visual_tags, \"color_palette\": c.color_palette\n",
        "        } for c in state.story_plan.characters\n",
        "    ]\n",
        "    frames_payload = [f.model_dump() for f in state.story_plan.frames]  # 이제 Pydantic 확정\n",
        "    refs = state.references\n",
        "    result = prompt_chain.invoke({\n",
        "        \"character_bible\": character_bible,\n",
        "        \"world\": state.story_plan.world.model_dump(),\n",
        "        \"style\": state.story_plan.style.model_dump(),\n",
        "        \"frames\": frames_payload,\n",
        "        \"width\": state.width, \"height\": state.height,\n",
        "        \"seed\": state.seed if state.seed is not None else \"none\",\n",
        "        \"references\": refs,\n",
        "        \"schema\": schema\n",
        "    })\n",
        "\n",
        "    # dict → ImagePrompt 안전 캐스팅\n",
        "    prompts = [ImagePrompt(**r) if isinstance(r, dict) else r for r in result]\n",
        "\n",
        "    # 기본값 보정\n",
        "    for p in prompts:\n",
        "        if not p.width:  p.width  = state.width\n",
        "        if not p.height: p.height = state.height\n",
        "        if p.seed is None: p.seed = state.seed\n",
        "    state.image_prompts = prompts\n",
        "    return state\n",
        "\n",
        "# -------- Gemini 이미지 생성 유틸 --------\n",
        "def _compose_gemini_prompt(p: ImagePrompt) -> str:\n",
        "    base = p.prompt.strip()\n",
        "    if p.negative_prompt:\n",
        "        base += \"\\n\\nDo NOT include (hard constraints): \" + p.negative_prompt.strip()\n",
        "    base += f\"\\n\\nOutput intent: single-panel comic frame. Target size approx {p.width}x{p.height}.\"\n",
        "    if p.seed is not None:\n",
        "        base += f\"\\nConsistency hint: keep character identity consistent (seed hint: {p.seed}).\"\n",
        "    return base\n",
        "\n",
        "def _generate_one_image_with_gemini(p: ImagePrompt):\n",
        "    contents = _compose_gemini_prompt(p)\n",
        "    resp = client.models.generate_content(\n",
        "        model=generator_name,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(response_modalities=['TEXT','IMAGE'])\n",
        "    )\n",
        "    text_parts = []\n",
        "    image_bytes = None\n",
        "    cand = resp.candidates[0]\n",
        "    for part in cand.content.parts:\n",
        "        if getattr(part, \"text\", None):\n",
        "            text_parts.append(part.text)\n",
        "        elif getattr(part, \"inline_data\", None):\n",
        "            data = part.inline_data.data\n",
        "            if isinstance(data, (bytes, bytearray)):\n",
        "                image_bytes = bytes(data)\n",
        "            else:\n",
        "                try:\n",
        "                    image_bytes = base64.b64decode(data)\n",
        "                except Exception:\n",
        "                    image_bytes = data\n",
        "    return image_bytes, (\"\\n\".join(text_parts) if text_parts else \"\")\n",
        "\n",
        "def node_image_gen(state: GraphState):\n",
        "    pathlib.Path(state.outputs_dir).mkdir(parents=True, exist_ok=True)\n",
        "    saved, texts = {}, {}\n",
        "    for p in state.image_prompts:\n",
        "        img_bytes, txt = _generate_one_image_with_gemini(p)\n",
        "        if img_bytes:\n",
        "            fname = f\"frame_{p.index:02d}.png\"\n",
        "            fpath = os.path.join(state.outputs_dir, fname)\n",
        "            Image.open(BytesIO(img_bytes)).save(fpath)\n",
        "            saved[p.index] = fpath\n",
        "        if txt:\n",
        "            texts[p.index] = txt\n",
        "    state.generated_images = saved\n",
        "    state.generation_texts = texts\n",
        "    return state"
      ],
      "metadata": {
        "id": "_DuiT_49cFkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------\n",
        "# 5) 실행 파이프라인\n",
        "# ---------------------------------------------------\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "def coerce_storyframe(item: Any, idx: int) -> StoryFrame:\n",
        "    \"\"\"LLM 반환물(StoryFrame|dict|str)을 안전하게 StoryFrame으로 캐스팅.\"\"\"\n",
        "    if isinstance(item, StoryFrame):\n",
        "        # index 보정\n",
        "        return item if item.index == idx else item.model_copy(update={\"index\": idx})\n",
        "\n",
        "    if isinstance(item, dict):\n",
        "        d = dict(item)\n",
        "        d.setdefault(\"index\", idx)\n",
        "        d.setdefault(\"summary\", d.get(\"summary\", \"\"))\n",
        "        d.setdefault(\"key_events\", d.get(\"key_events\", []))\n",
        "        d.setdefault(\"characters\", d.get(\"characters\", []))\n",
        "        d.setdefault(\"emotion\", d.get(\"emotion\", \"\"))\n",
        "        d.setdefault(\"camera\", d.get(\"camera\", \"medium shot\"))\n",
        "        d.setdefault(\"lighting\", d.get(\"lighting\", \"soft light\"))\n",
        "        d.setdefault(\"palette\", d.get(\"palette\", []))\n",
        "        d.setdefault(\"speech_bubbles\", d.get(\"speech_bubbles\", []))\n",
        "        return StoryFrame(**d)\n",
        "\n",
        "    if isinstance(item, str):\n",
        "        s = item.strip()\n",
        "        # JSON 문자열일 수도 있음\n",
        "        if s and (s.startswith(\"{\") or s.startswith(\"[\")):\n",
        "            try:\n",
        "                obj = json.loads(s)\n",
        "                return coerce_storyframe(obj, idx)\n",
        "            except Exception:\n",
        "                pass\n",
        "        # 그냥 텍스트 요약으로 간주\n",
        "        return StoryFrame(\n",
        "            index=idx,\n",
        "            summary=s,\n",
        "            key_events=[],\n",
        "            characters=[],\n",
        "            emotion=\"\",\n",
        "            camera=\"medium shot\",\n",
        "            lighting=\"soft light\",\n",
        "            palette=[],\n",
        "            speech_bubbles=[]\n",
        "        )\n",
        "\n",
        "    # 기타 타입은 오류 대신 최소 골격으로 대체\n",
        "    return StoryFrame(\n",
        "        index=idx,\n",
        "        summary=\"\",\n",
        "        key_events=[],\n",
        "        characters=[],\n",
        "        emotion=\"\",\n",
        "        camera=\"medium shot\",\n",
        "        lighting=\"soft light\",\n",
        "        palette=[],\n",
        "        speech_bubbles=[]\n",
        "    )\n",
        "\n",
        "def normalize_state(state_dict) -> GraphState:\n",
        "    \"\"\"LangGraph가 반환한 dict를 GraphState 및 내부 Pydantic 모델들로 캐스팅.\"\"\"\n",
        "    if isinstance(state_dict, GraphState):\n",
        "        return state_dict\n",
        "\n",
        "    s = dict(state_dict)  # shallow copy\n",
        "\n",
        "    # story_plan 보정\n",
        "    sp = s.get(\"story_plan\")\n",
        "\n",
        "    if isinstance(sp, dict):\n",
        "        # 내부 필드도 전부 캐스팅\n",
        "        world = sp.get(\"world\")\n",
        "        style = sp.get(\"style\")\n",
        "        chars = sp.get(\"characters\", [])\n",
        "        frames = sp.get(\"frames\", [])\n",
        "\n",
        "        sp_obj = StoryPlan(\n",
        "            title=sp.get(\"title\", \"\"),\n",
        "            synopsis=sp.get(\"synopsis\", \"\"),\n",
        "            world=WorldSetting(**world) if isinstance(world, dict) else world,\n",
        "            style=StyleGuide(**style) if isinstance(style, dict) else style,\n",
        "            characters=[Character(**c) if isinstance(c, dict) else c for c in chars],\n",
        "            frames=[StoryFrame(**f) if isinstance(f, dict) else f for f in frames],\n",
        "        )\n",
        "        s[\"story_plan\"] = sp_obj\n",
        "\n",
        "    # image_prompts 보정\n",
        "    ip = s.get(\"image_prompts\") or []\n",
        "    s[\"image_prompts\"] = [ImagePrompt(**p) if isinstance(p, dict) else p for p in ip]\n",
        "\n",
        "    # 나머지는 GraphState 생성자가 기본값 처리\n",
        "    return GraphState(**s)\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    user_input: str = \"\",\n",
        "    keywords: Optional[List[str]] = None,\n",
        "    n: int = 4,\n",
        "    genre: str = \"adventure\",\n",
        "    audience: str = \"teen\",\n",
        "    tone: str = \"warm and witty\",\n",
        "    paragraphs: int = 8,\n",
        "    width: int = 512,\n",
        "    height: int = 512,\n",
        "    seed: Optional[int] = 1234,\n",
        "    references: Optional[Dict[str, List[str]]] = None,\n",
        "    outputs_dir: str = \"outputs\"\n",
        ") -> tuple[PipelineOutput, Dict[int,str]]:\n",
        "    init = GraphState(\n",
        "        user_input=user_input or \"키워드 기반 스토리 작성\",\n",
        "        keywords=keywords or [],\n",
        "        n=n, genre=genre, audience=audience, tone=tone, paragraphs=paragraphs,\n",
        "        width=width, height=height, seed=seed, references=references or {},\n",
        "        outputs_dir=outputs_dir\n",
        "    )\n",
        "    raw_state = app.invoke(init)               # dict가 올 수 있음\n",
        "    final_state = normalize_state(raw_state)   # 캐스팅\n",
        "\n",
        "    return PipelineOutput(\n",
        "        story=final_state.story_plan,\n",
        "        image_prompts=final_state.image_prompts\n",
        "    ), final_state.generated_images"
      ],
      "metadata": {
        "id": "qDMRw9UlcHs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 무료 API\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAVSJKpIMURFnccjK3RcJXUavUUcUOOsts\"\n",
        "\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.7)\n",
        "\n",
        "generator_name = \"gemini-2.0-flash-preview-image-generation\"\n",
        "client = genai.Client()\n",
        "\n",
        "# 그래프 구성\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"story_and_bible\", node_story_and_bible)\n",
        "graph.add_node(\"scene_split\", node_scene_split)\n",
        "graph.add_node(\"prompt_gen\", node_prompt_gen)\n",
        "graph.add_node(\"image_gen\", node_image_gen)\n",
        "\n",
        "graph.set_entry_point(\"story_and_bible\")\n",
        "graph.add_edge(\"story_and_bible\", \"scene_split\")\n",
        "graph.add_edge(\"scene_split\", \"prompt_gen\")\n",
        "graph.add_edge(\"prompt_gen\", \"image_gen\")\n",
        "graph.add_edge(\"image_gen\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "user_input = \"슈퍼맨이 공중에 떠서 정글을 탐험하는 스토리\"\n",
        "keywords = [\"정글\", \"신비\"]\n",
        "genre = \"adventure\"\n",
        "audience = \"all-ages\"\n",
        "tone = \"adventurous, slightly humorous\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 파이프라인 실행 (스토리 + 프롬프트 + 이미지 생성까지)\n",
        "    out, generated_images = run_pipeline(\n",
        "        user_input=user_input,\n",
        "        keywords=keywords,\n",
        "        n=4,\n",
        "        genre=genre,\n",
        "        audience=audience,\n",
        "        tone=tone,\n",
        "        paragraphs=8, # 스토리 문단 제어\n",
        "        seed=20240814,\n",
        "        outputs_dir=\"outputs\"\n",
        "    )\n",
        "\n",
        "    print(\"제목:\", out.story.title)\n",
        "    print(\"장면 요약:\")\n",
        "    for f in out.story.frames:\n",
        "        print(f\"[{f.index}] {f.summary}\")\n",
        "\n",
        "    state = normalize_state(app.invoke(GraphState(\n",
        "        user_input=\"same\", keywords=[], n=1\n",
        "    )))"
      ],
      "metadata": {
        "id": "Z_0tJkUdeOoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# 파일 목록\n",
        "files = [\"outputs/frame_01.png\", \"outputs/frame_02.png\", \"outputs/frame_03.png\", \"outputs/frame_04.png\"]\n",
        "frames = []\n",
        "for fname in files:\n",
        "    img = Image.open(fname)   # 이미지 열기\n",
        "    frames.append(img)\n",
        "    # img.save(\"copy_\" + fname)  # 복사 저장 가능"
      ],
      "metadata": {
        "id": "j1VW2polliz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames[0]"
      ],
      "metadata": {
        "id": "4AkYVPNrlldQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames[1]"
      ],
      "metadata": {
        "id": "mri6RrosGRj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames[2]"
      ],
      "metadata": {
        "id": "q2-0SGCOHGFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames[3]"
      ],
      "metadata": {
        "id": "HxToS-dpHHDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}