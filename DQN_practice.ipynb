{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongheechoi/education/blob/main/DQN_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taekc1AG9eRT"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install gym==0.25.2\n",
        "!pip install matplotlib\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLbU9X7cF7q"
      },
      "source": [
        "목표: 알고리즘을 완성하고 높은 test score 성능 달성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMLmNgHHrNl8"
      },
      "source": [
        "수행 내용:\n",
        "1. 아래 DQN 코드에 10개의 빈칸 라인을 채워 코드를 구동시키세요.\n",
        "  - 빈칸 부분은 주석으로 표시되어 있으며 1줄의 라인을 채우시면 됩니다.\n",
        "2. 필요한 경우, 더 높은 성능을 달성하기 위해 hyperparameter조정, Q 네트워크 구조 변경 등 적절히 조절하세요.\n",
        "\n",
        "참고 사항:\n",
        "- 최종 점수는 마지막 10번의 test score 평균으로 계산합니다.\n",
        "- Acrobot 환경의 특성 상 test score는 음수가 나오게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiyLHpVGETqc",
        "outputId": "dc6a69a5-2ee8-4393-8722-ea098db26c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[time:100]test score: -500.0, train loss: 0.002, epsilon: 0.100\n",
            "[time:200]test score: -500.0, train loss: 0.011, epsilon: 0.100\n",
            "[time:300]test score: -500.0, train loss: 0.005, epsilon: 0.100\n",
            "[time:400]test score: -500.0, train loss: 0.006, epsilon: 0.100\n",
            "[time:500]test score: -500.0, train loss: 0.066, epsilon: 0.100\n",
            "[time:600]test score: -500.0, train loss: 0.180, epsilon: 0.100\n",
            "[time:700]test score: -500.0, train loss: 0.019, epsilon: 0.100\n",
            "[time:800]test score: -500.0, train loss: 0.128, epsilon: 0.100\n",
            "[time:900]test score: -500.0, train loss: 0.010, epsilon: 0.099\n",
            "[time:1000]test score: -267.0, train loss: 0.025, epsilon: 0.099\n",
            "[time:1100]test score: -481.0, train loss: 0.028, epsilon: 0.099\n",
            "[time:1200]test score: -500.0, train loss: 1.140, epsilon: 0.099\n",
            "[time:1300]test score: -500.0, train loss: 0.023, epsilon: 0.099\n",
            "[time:1400]test score: -242.0, train loss: 0.018, epsilon: 0.099\n",
            "[time:1500]test score: -500.0, train loss: 0.031, epsilon: 0.099\n",
            "[time:1600]test score: -500.0, train loss: 1.314, epsilon: 0.099\n",
            "[time:1700]test score: -213.0, train loss: 0.044, epsilon: 0.098\n",
            "[time:1800]test score: -251.0, train loss: 0.034, epsilon: 0.098\n",
            "[time:1900]test score: -154.0, train loss: 0.071, epsilon: 0.098\n",
            "[time:2000]test score: -500.0, train loss: 0.114, epsilon: 0.098\n",
            "[time:2100]test score: -500.0, train loss: 0.055, epsilon: 0.098\n",
            "[time:2200]test score: -500.0, train loss: 0.044, epsilon: 0.097\n",
            "[time:2300]test score: -500.0, train loss: 0.047, epsilon: 0.097\n",
            "[time:2400]test score: -500.0, train loss: 2.566, epsilon: 0.097\n",
            "[time:2500]test score: -129.0, train loss: 0.176, epsilon: 0.097\n",
            "[time:2600]test score: -500.0, train loss: 2.539, epsilon: 0.097\n",
            "[time:2700]test score: -500.0, train loss: 2.803, epsilon: 0.096\n",
            "[time:2800]test score: -500.0, train loss: 0.324, epsilon: 0.096\n",
            "[time:2900]test score: -500.0, train loss: 0.094, epsilon: 0.096\n",
            "[time:3000]test score: -154.0, train loss: 0.158, epsilon: 0.096\n",
            "[time:3100]test score: -500.0, train loss: 1.700, epsilon: 0.096\n",
            "[time:3200]test score: -108.0, train loss: 0.263, epsilon: 0.096\n",
            "[time:3300]test score: -500.0, train loss: 3.684, epsilon: 0.096\n",
            "[time:3400]test score: -500.0, train loss: 0.177, epsilon: 0.095\n",
            "[time:3500]test score: -500.0, train loss: 0.088, epsilon: 0.095\n",
            "[time:3600]test score: -170.0, train loss: 1.928, epsilon: 0.095\n",
            "[time:3700]test score: -500.0, train loss: 0.150, epsilon: 0.094\n",
            "[time:3800]test score: -99.0, train loss: 0.428, epsilon: 0.094\n",
            "[time:3900]test score: -78.0, train loss: 0.228, epsilon: 0.094\n",
            "[time:4000]test score: -500.0, train loss: 0.128, epsilon: 0.093\n",
            "[time:4100]test score: -323.0, train loss: 0.154, epsilon: 0.093\n",
            "[time:4200]test score: -500.0, train loss: 0.301, epsilon: 0.093\n",
            "[time:4300]test score: -500.0, train loss: 0.186, epsilon: 0.093\n",
            "[time:4400]test score: -500.0, train loss: 3.163, epsilon: 0.093\n",
            "[time:4500]test score: -199.0, train loss: 6.142, epsilon: 0.093\n",
            "[time:4600]test score: -139.0, train loss: 0.335, epsilon: 0.093\n",
            "[time:4700]test score: -101.0, train loss: 0.200, epsilon: 0.092\n",
            "[time:4800]test score: -500.0, train loss: 0.471, epsilon: 0.092\n",
            "[time:4900]test score: -500.0, train loss: 0.465, epsilon: 0.092\n",
            "[time:5000]test score: -500.0, train loss: 0.413, epsilon: 0.092\n",
            "[time:5100]test score: -145.0, train loss: 0.361, epsilon: 0.091\n",
            "[time:5200]test score: -500.0, train loss: 2.571, epsilon: 0.091\n",
            "[time:5300]test score: -500.0, train loss: 10.034, epsilon: 0.091\n",
            "[time:5400]test score: -500.0, train loss: 0.624, epsilon: 0.090\n",
            "[time:5500]test score: -500.0, train loss: 0.330, epsilon: 0.090\n",
            "[time:5600]test score: -500.0, train loss: 6.344, epsilon: 0.090\n",
            "[time:5700]test score: -500.0, train loss: 0.353, epsilon: 0.090\n",
            "[time:5800]test score: -137.0, train loss: 0.292, epsilon: 0.090\n",
            "[time:5900]test score: -500.0, train loss: 0.530, epsilon: 0.089\n",
            "[time:6000]test score: -500.0, train loss: 9.080, epsilon: 0.089\n",
            "[time:6100]test score: -500.0, train loss: 0.742, epsilon: 0.089\n",
            "[time:6200]test score: -126.0, train loss: 9.559, epsilon: 0.088\n",
            "[time:6300]test score: -500.0, train loss: 0.442, epsilon: 0.088\n",
            "[time:6400]test score: -143.0, train loss: 0.619, epsilon: 0.088\n",
            "[time:6500]test score: -500.0, train loss: 0.624, epsilon: 0.087\n",
            "[time:6600]test score: -95.0, train loss: 0.698, epsilon: 0.087\n",
            "[time:6700]test score: -110.0, train loss: 1.476, epsilon: 0.087\n",
            "[time:6800]test score: -138.0, train loss: 0.939, epsilon: 0.087\n",
            "[time:6900]test score: -119.0, train loss: 0.740, epsilon: 0.087\n",
            "[time:7000]test score: -500.0, train loss: 0.650, epsilon: 0.086\n",
            "[time:7100]test score: -131.0, train loss: 9.080, epsilon: 0.086\n",
            "[time:7200]test score: -115.0, train loss: 0.499, epsilon: 0.086\n",
            "[time:7300]test score: -85.0, train loss: 0.817, epsilon: 0.086\n",
            "[time:7400]test score: -500.0, train loss: 0.564, epsilon: 0.085\n",
            "[time:7500]test score: -96.0, train loss: 0.607, epsilon: 0.085\n",
            "[time:7600]test score: -106.0, train loss: 0.474, epsilon: 0.084\n",
            "[time:7700]test score: -500.0, train loss: 1.051, epsilon: 0.084\n",
            "[time:7800]test score: -134.0, train loss: 0.660, epsilon: 0.084\n",
            "[time:7900]test score: -113.0, train loss: 8.135, epsilon: 0.084\n",
            "[time:8000]test score: -92.0, train loss: 1.146, epsilon: 0.083\n",
            "[time:8100]test score: -97.0, train loss: 0.589, epsilon: 0.083\n",
            "[time:8200]test score: -94.0, train loss: 1.010, epsilon: 0.083\n",
            "[time:8300]test score: -86.0, train loss: 0.612, epsilon: 0.082\n",
            "[time:8400]test score: -500.0, train loss: 0.652, epsilon: 0.082\n",
            "[time:8500]test score: -500.0, train loss: 0.384, epsilon: 0.081\n",
            "[time:8600]test score: -85.0, train loss: 0.671, epsilon: 0.081\n",
            "[time:8700]test score: -77.0, train loss: 0.397, epsilon: 0.081\n",
            "[time:8800]test score: -75.0, train loss: 0.709, epsilon: 0.080\n",
            "[time:8900]test score: -110.0, train loss: 1.618, epsilon: 0.080\n",
            "[time:9000]test score: -78.0, train loss: 0.949, epsilon: 0.079\n",
            "[time:9100]test score: -70.0, train loss: 0.930, epsilon: 0.079\n",
            "[time:9200]test score: -500.0, train loss: 1.176, epsilon: 0.079\n",
            "[time:9300]test score: -72.0, train loss: 1.907, epsilon: 0.078\n",
            "[time:9400]test score: -72.0, train loss: 0.773, epsilon: 0.078\n",
            "[time:9500]test score: -80.0, train loss: 1.806, epsilon: 0.077\n",
            "[time:9600]test score: -85.0, train loss: 0.807, epsilon: 0.077\n",
            "[time:9700]test score: -500.0, train loss: 0.703, epsilon: 0.077\n",
            "[time:9800]test score: -133.0, train loss: 0.668, epsilon: 0.077\n",
            "[time:9900]test score: -123.0, train loss: 1.253, epsilon: 0.076\n",
            "[time:10000]test score: -90.0, train loss: 1.247, epsilon: 0.076\n",
            "[time:10100]test score: -62.0, train loss: 1.125, epsilon: 0.075\n",
            "[time:10200]test score: -90.0, train loss: 1.731, epsilon: 0.075\n",
            "[time:10300]test score: -500.0, train loss: 0.980, epsilon: 0.074\n",
            "[time:10400]test score: -103.0, train loss: 1.821, epsilon: 0.074\n",
            "[time:10500]test score: -500.0, train loss: 1.216, epsilon: 0.074\n",
            "[time:10600]test score: -500.0, train loss: 1.125, epsilon: 0.073\n",
            "[time:10700]test score: -70.0, train loss: 1.570, epsilon: 0.073\n",
            "[time:10800]test score: -102.0, train loss: 1.348, epsilon: 0.072\n",
            "[time:10900]test score: -500.0, train loss: 0.993, epsilon: 0.072\n",
            "[time:11000]test score: -100.0, train loss: 0.957, epsilon: 0.071\n",
            "[time:11100]test score: -193.0, train loss: 0.818, epsilon: 0.071\n",
            "[time:11200]test score: -94.0, train loss: 1.314, epsilon: 0.071\n",
            "[time:11300]test score: -91.0, train loss: 0.846, epsilon: 0.070\n",
            "[time:11400]test score: -90.0, train loss: 1.196, epsilon: 0.070\n",
            "[time:11500]test score: -500.0, train loss: 1.249, epsilon: 0.070\n",
            "[time:11600]test score: -500.0, train loss: 1.121, epsilon: 0.069\n",
            "[time:11700]test score: -160.0, train loss: 1.132, epsilon: 0.069\n",
            "[time:11800]test score: -76.0, train loss: 1.024, epsilon: 0.069\n",
            "[time:11900]test score: -91.0, train loss: 0.996, epsilon: 0.068\n",
            "[time:12000]test score: -70.0, train loss: 1.224, epsilon: 0.068\n",
            "[time:12100]test score: -298.0, train loss: 0.906, epsilon: 0.067\n",
            "[time:12200]test score: -87.0, train loss: 1.254, epsilon: 0.067\n",
            "[time:12300]test score: -85.0, train loss: 0.972, epsilon: 0.067\n",
            "[time:12400]test score: -75.0, train loss: 1.633, epsilon: 0.066\n",
            "[time:12500]test score: -70.0, train loss: 1.216, epsilon: 0.066\n",
            "[time:12600]test score: -500.0, train loss: 2.625, epsilon: 0.066\n",
            "[time:12700]test score: -86.0, train loss: 0.936, epsilon: 0.065\n",
            "[time:12800]test score: -500.0, train loss: 0.956, epsilon: 0.065\n",
            "[time:12900]test score: -500.0, train loss: 0.658, epsilon: 0.065\n",
            "[time:13000]test score: -94.0, train loss: 1.574, epsilon: 0.064\n",
            "[time:13100]test score: -63.0, train loss: 0.757, epsilon: 0.064\n",
            "[time:13200]test score: -73.0, train loss: 1.030, epsilon: 0.064\n",
            "[time:13300]test score: -79.0, train loss: 1.730, epsilon: 0.063\n",
            "[time:13400]test score: -95.0, train loss: 1.377, epsilon: 0.063\n",
            "[time:13500]test score: -102.0, train loss: 0.794, epsilon: 0.062\n",
            "[time:13600]test score: -79.0, train loss: 0.920, epsilon: 0.062\n",
            "[time:13700]test score: -91.0, train loss: 2.099, epsilon: 0.062\n",
            "[time:13800]test score: -85.0, train loss: 0.851, epsilon: 0.061\n",
            "[time:13900]test score: -106.0, train loss: 0.958, epsilon: 0.061\n",
            "[time:14000]test score: -92.0, train loss: 1.178, epsilon: 0.061\n",
            "[time:14100]test score: -87.0, train loss: 0.989, epsilon: 0.060\n",
            "[time:14200]test score: -102.0, train loss: 1.917, epsilon: 0.060\n",
            "[time:14300]test score: -78.0, train loss: 1.018, epsilon: 0.060\n",
            "[time:14400]test score: -70.0, train loss: 1.250, epsilon: 0.059\n",
            "[time:14500]test score: -85.0, train loss: 1.486, epsilon: 0.059\n",
            "[time:14600]test score: -71.0, train loss: 0.965, epsilon: 0.059\n",
            "[time:14700]test score: -77.0, train loss: 0.614, epsilon: 0.058\n",
            "[time:14800]test score: -92.0, train loss: 1.262, epsilon: 0.058\n",
            "[time:14900]test score: -74.0, train loss: 2.212, epsilon: 0.058\n",
            "[time:15000]test score: -125.0, train loss: 1.071, epsilon: 0.058\n",
            "[time:15100]test score: -117.0, train loss: 1.279, epsilon: 0.057\n",
            "[time:15200]test score: -132.0, train loss: 0.732, epsilon: 0.057\n",
            "[time:15300]test score: -85.0, train loss: 0.470, epsilon: 0.056\n",
            "[time:15400]test score: -84.0, train loss: 1.292, epsilon: 0.056\n",
            "[time:15500]test score: -78.0, train loss: 1.366, epsilon: 0.056\n",
            "[time:15600]test score: -93.0, train loss: 0.922, epsilon: 0.056\n",
            "[time:15700]test score: -174.0, train loss: 1.085, epsilon: 0.055\n",
            "[time:15800]test score: -72.0, train loss: 0.653, epsilon: 0.055\n",
            "[time:15900]test score: -86.0, train loss: 0.805, epsilon: 0.055\n",
            "[time:16000]test score: -74.0, train loss: 1.117, epsilon: 0.055\n",
            "[time:16100]test score: -81.0, train loss: 1.018, epsilon: 0.054\n",
            "[time:16200]test score: -70.0, train loss: 0.804, epsilon: 0.054\n",
            "[time:16300]test score: -78.0, train loss: 0.673, epsilon: 0.053\n",
            "[time:16400]test score: -100.0, train loss: 0.725, epsilon: 0.053\n",
            "[time:16500]test score: -71.0, train loss: 0.681, epsilon: 0.053\n",
            "[time:16600]test score: -81.0, train loss: 0.678, epsilon: 0.053\n",
            "[time:16700]test score: -63.0, train loss: 0.712, epsilon: 0.052\n",
            "[time:16800]test score: -74.0, train loss: 0.991, epsilon: 0.052\n",
            "[time:16900]test score: -69.0, train loss: 1.263, epsilon: 0.052\n",
            "[time:17000]test score: -83.0, train loss: 1.085, epsilon: 0.051\n",
            "[time:17100]test score: -76.0, train loss: 0.579, epsilon: 0.051\n",
            "[time:17200]test score: -73.0, train loss: 1.166, epsilon: 0.051\n",
            "[time:17300]test score: -70.0, train loss: 0.432, epsilon: 0.051\n",
            "[time:17400]test score: -86.0, train loss: 0.606, epsilon: 0.050\n",
            "[time:17500]test score: -85.0, train loss: 0.922, epsilon: 0.050\n",
            "[time:17600]test score: -144.0, train loss: 0.742, epsilon: 0.050\n",
            "[time:17700]test score: -75.0, train loss: 1.013, epsilon: 0.049\n",
            "[time:17800]test score: -79.0, train loss: 1.076, epsilon: 0.049\n",
            "[time:17900]test score: -86.0, train loss: 0.787, epsilon: 0.049\n",
            "[time:18000]test score: -95.0, train loss: 0.423, epsilon: 0.049\n",
            "[time:18100]test score: -73.0, train loss: 0.395, epsilon: 0.048\n",
            "[time:18200]test score: -62.0, train loss: 0.882, epsilon: 0.048\n",
            "[time:18300]test score: -133.0, train loss: 0.494, epsilon: 0.048\n",
            "[time:18400]test score: -74.0, train loss: 0.802, epsilon: 0.047\n",
            "[time:18500]test score: -70.0, train loss: 0.878, epsilon: 0.047\n",
            "[time:18600]test score: -70.0, train loss: 0.893, epsilon: 0.047\n",
            "[time:18700]test score: -78.0, train loss: 0.673, epsilon: 0.046\n",
            "[time:18800]test score: -109.0, train loss: 0.555, epsilon: 0.046\n",
            "[time:18900]test score: -77.0, train loss: 0.723, epsilon: 0.046\n",
            "[time:19000]test score: -81.0, train loss: 0.596, epsilon: 0.046\n",
            "[time:19100]test score: -78.0, train loss: 0.749, epsilon: 0.045\n",
            "[time:19200]test score: -71.0, train loss: 0.745, epsilon: 0.045\n",
            "[time:19300]test score: -145.0, train loss: 0.607, epsilon: 0.045\n",
            "[time:19400]test score: -86.0, train loss: 0.359, epsilon: 0.045\n",
            "[time:19500]test score: -88.0, train loss: 1.106, epsilon: 0.045\n",
            "[time:19600]test score: -90.0, train loss: 0.561, epsilon: 0.044\n",
            "[time:19700]test score: -96.0, train loss: 0.901, epsilon: 0.044\n",
            "[time:19800]test score: -70.0, train loss: 0.767, epsilon: 0.044\n",
            "[time:19900]test score: -123.0, train loss: 0.311, epsilon: 0.044\n",
            "[time:20000]test score: -72.0, train loss: 1.355, epsilon: 0.043\n",
            "[time:20100]test score: -92.0, train loss: 0.644, epsilon: 0.043\n",
            "[time:20200]test score: -75.0, train loss: 1.618, epsilon: 0.043\n",
            "[time:20300]test score: -79.0, train loss: 0.664, epsilon: 0.042\n",
            "[time:20400]test score: -94.0, train loss: 0.717, epsilon: 0.042\n",
            "[time:20500]test score: -104.0, train loss: 0.903, epsilon: 0.042\n",
            "[time:20600]test score: -94.0, train loss: 0.937, epsilon: 0.042\n",
            "[time:20700]test score: -70.0, train loss: 0.517, epsilon: 0.042\n",
            "[time:20800]test score: -84.0, train loss: 0.762, epsilon: 0.041\n",
            "[time:20900]test score: -72.0, train loss: 0.673, epsilon: 0.041\n",
            "[time:21000]test score: -75.0, train loss: 0.412, epsilon: 0.041\n",
            "[time:21100]test score: -101.0, train loss: 0.805, epsilon: 0.041\n",
            "[time:21200]test score: -75.0, train loss: 0.623, epsilon: 0.041\n",
            "[time:21300]test score: -109.0, train loss: 0.551, epsilon: 0.040\n",
            "[time:21400]test score: -64.0, train loss: 0.613, epsilon: 0.040\n",
            "[time:21500]test score: -407.0, train loss: 0.851, epsilon: 0.040\n",
            "[time:21600]test score: -76.0, train loss: 0.844, epsilon: 0.040\n",
            "[time:21700]test score: -88.0, train loss: 0.910, epsilon: 0.040\n",
            "[time:21800]test score: -74.0, train loss: 0.493, epsilon: 0.039\n",
            "[time:21900]test score: -73.0, train loss: 0.534, epsilon: 0.039\n",
            "[time:22000]test score: -86.0, train loss: 0.616, epsilon: 0.039\n",
            "[time:22100]test score: -77.0, train loss: 0.865, epsilon: 0.039\n",
            "[time:22200]test score: -72.0, train loss: 0.483, epsilon: 0.038\n",
            "[time:22300]test score: -100.0, train loss: 1.355, epsilon: 0.038\n",
            "[time:22400]test score: -70.0, train loss: 0.536, epsilon: 0.038\n",
            "[time:22500]test score: -77.0, train loss: 0.532, epsilon: 0.038\n",
            "[time:22600]test score: -91.0, train loss: 0.444, epsilon: 0.038\n",
            "[time:22700]test score: -62.0, train loss: 0.716, epsilon: 0.037\n",
            "[time:22800]test score: -103.0, train loss: 1.205, epsilon: 0.037\n",
            "[time:22900]test score: -70.0, train loss: 0.521, epsilon: 0.037\n",
            "[time:23000]test score: -75.0, train loss: 0.661, epsilon: 0.037\n",
            "[time:23100]test score: -95.0, train loss: 0.829, epsilon: 0.037\n",
            "[time:23200]test score: -71.0, train loss: 0.496, epsilon: 0.036\n",
            "[time:23300]test score: -102.0, train loss: 0.607, epsilon: 0.036\n",
            "[time:23400]test score: -78.0, train loss: 0.768, epsilon: 0.036\n",
            "[time:23500]test score: -100.0, train loss: 0.745, epsilon: 0.036\n",
            "[time:23600]test score: -91.0, train loss: 0.629, epsilon: 0.035\n",
            "[time:23700]test score: -75.0, train loss: 0.416, epsilon: 0.035\n",
            "[time:23800]test score: -100.0, train loss: 0.951, epsilon: 0.035\n",
            "[time:23900]test score: -73.0, train loss: 0.703, epsilon: 0.035\n",
            "[time:24000]test score: -81.0, train loss: 0.396, epsilon: 0.035\n",
            "[time:24100]test score: -98.0, train loss: 0.440, epsilon: 0.034\n",
            "[time:24200]test score: -78.0, train loss: 0.860, epsilon: 0.034\n",
            "[time:24300]test score: -99.0, train loss: 1.621, epsilon: 0.034\n",
            "[time:24400]test score: -83.0, train loss: 0.654, epsilon: 0.034\n",
            "[time:24500]test score: -107.0, train loss: 1.255, epsilon: 0.034\n",
            "[time:24600]test score: -85.0, train loss: 0.708, epsilon: 0.033\n",
            "[time:24700]test score: -75.0, train loss: 0.511, epsilon: 0.033\n",
            "[time:24800]test score: -90.0, train loss: 0.838, epsilon: 0.033\n",
            "[time:24900]test score: -87.0, train loss: 0.567, epsilon: 0.033\n",
            "[time:25000]test score: -64.0, train loss: 0.658, epsilon: 0.033\n",
            "[time:25100]test score: -72.0, train loss: 0.681, epsilon: 0.033\n",
            "[time:25200]test score: -74.0, train loss: 1.421, epsilon: 0.032\n",
            "[time:25300]test score: -97.0, train loss: 0.944, epsilon: 0.032\n",
            "[time:25400]test score: -92.0, train loss: 1.113, epsilon: 0.032\n",
            "[time:25500]test score: -98.0, train loss: 0.549, epsilon: 0.032\n",
            "[time:25600]test score: -119.0, train loss: 0.909, epsilon: 0.032\n",
            "[time:25700]test score: -92.0, train loss: 0.801, epsilon: 0.031\n",
            "[time:25800]test score: -62.0, train loss: 1.545, epsilon: 0.031\n",
            "[time:25900]test score: -77.0, train loss: 0.948, epsilon: 0.031\n",
            "[time:26000]test score: -75.0, train loss: 0.876, epsilon: 0.031\n",
            "[time:26100]test score: -68.0, train loss: 0.726, epsilon: 0.031\n",
            "[time:26200]test score: -105.0, train loss: 1.779, epsilon: 0.031\n",
            "[time:26300]test score: -75.0, train loss: 0.866, epsilon: 0.030\n",
            "[time:26400]test score: -500.0, train loss: 0.611, epsilon: 0.030\n",
            "[time:26500]test score: -74.0, train loss: 0.854, epsilon: 0.030\n",
            "[time:26600]test score: -74.0, train loss: 0.973, epsilon: 0.030\n",
            "[time:26700]test score: -76.0, train loss: 1.225, epsilon: 0.030\n",
            "[time:26800]test score: -75.0, train loss: 0.599, epsilon: 0.030\n",
            "[time:26900]test score: -95.0, train loss: 0.465, epsilon: 0.029\n",
            "[time:27000]test score: -70.0, train loss: 0.852, epsilon: 0.029\n",
            "[time:27100]test score: -78.0, train loss: 0.873, epsilon: 0.029\n",
            "[time:27200]test score: -75.0, train loss: 1.537, epsilon: 0.029\n",
            "[time:27300]test score: -84.0, train loss: 0.648, epsilon: 0.029\n",
            "[time:27400]test score: -74.0, train loss: 0.973, epsilon: 0.028\n",
            "[time:27500]test score: -86.0, train loss: 1.277, epsilon: 0.028\n",
            "[time:27600]test score: -78.0, train loss: 1.059, epsilon: 0.028\n",
            "[time:27700]test score: -100.0, train loss: 0.618, epsilon: 0.028\n",
            "[time:27800]test score: -70.0, train loss: 0.676, epsilon: 0.028\n",
            "[time:27900]test score: -71.0, train loss: 1.476, epsilon: 0.028\n",
            "[time:28000]test score: -86.0, train loss: 0.658, epsilon: 0.027\n",
            "[time:28100]test score: -95.0, train loss: 1.030, epsilon: 0.027\n",
            "[time:28200]test score: -70.0, train loss: 0.444, epsilon: 0.027\n",
            "[time:28300]test score: -75.0, train loss: 1.148, epsilon: 0.027\n",
            "[time:28400]test score: -128.0, train loss: 0.490, epsilon: 0.027\n",
            "[time:28500]test score: -74.0, train loss: 0.855, epsilon: 0.027\n",
            "[time:28600]test score: -76.0, train loss: 0.681, epsilon: 0.026\n",
            "[time:28700]test score: -73.0, train loss: 1.165, epsilon: 0.026\n",
            "[time:28800]test score: -70.0, train loss: 0.513, epsilon: 0.026\n",
            "[time:28900]test score: -114.0, train loss: 0.649, epsilon: 0.026\n",
            "[time:29000]test score: -83.0, train loss: 0.650, epsilon: 0.026\n",
            "[time:29100]test score: -82.0, train loss: 0.695, epsilon: 0.026\n",
            "[time:29200]test score: -76.0, train loss: 0.474, epsilon: 0.025\n",
            "[time:29300]test score: -90.0, train loss: 0.639, epsilon: 0.025\n",
            "[time:29400]test score: -70.0, train loss: 0.523, epsilon: 0.025\n",
            "[time:29500]test score: -83.0, train loss: 1.241, epsilon: 0.025\n",
            "[time:29600]test score: -127.0, train loss: 0.748, epsilon: 0.025\n",
            "[time:29700]test score: -61.0, train loss: 0.919, epsilon: 0.025\n",
            "[time:29800]test score: -71.0, train loss: 0.631, epsilon: 0.024\n",
            "[time:29900]test score: -75.0, train loss: 0.827, epsilon: 0.024\n",
            "[time:30000]test score: -110.0, train loss: 1.203, epsilon: 0.024\n",
            "[time:30100]test score: -121.0, train loss: 0.544, epsilon: 0.024\n",
            "[time:30200]test score: -84.0, train loss: 0.903, epsilon: 0.024\n",
            "[time:30300]test score: -500.0, train loss: 0.629, epsilon: 0.024\n",
            "[time:30400]test score: -128.0, train loss: 0.729, epsilon: 0.024\n",
            "[time:30500]test score: -93.0, train loss: 1.035, epsilon: 0.023\n",
            "[time:30600]test score: -72.0, train loss: 0.728, epsilon: 0.023\n",
            "[time:30700]test score: -98.0, train loss: 0.642, epsilon: 0.023\n",
            "[time:30800]test score: -74.0, train loss: 0.878, epsilon: 0.023\n",
            "[time:30900]test score: -94.0, train loss: 0.489, epsilon: 0.023\n",
            "[time:31000]test score: -63.0, train loss: 0.993, epsilon: 0.023\n",
            "[time:31100]test score: -89.0, train loss: 0.595, epsilon: 0.023\n",
            "[time:31200]test score: -87.0, train loss: 0.364, epsilon: 0.023\n",
            "[time:31300]test score: -77.0, train loss: 0.403, epsilon: 0.022\n",
            "[time:31400]test score: -74.0, train loss: 0.519, epsilon: 0.022\n",
            "[time:31500]test score: -86.0, train loss: 0.600, epsilon: 0.022\n",
            "[time:31600]test score: -89.0, train loss: 1.063, epsilon: 0.022\n",
            "[time:31700]test score: -89.0, train loss: 0.591, epsilon: 0.022\n",
            "[time:31800]test score: -101.0, train loss: 0.603, epsilon: 0.022\n",
            "[time:31900]test score: -70.0, train loss: 0.550, epsilon: 0.022\n",
            "[time:32000]test score: -77.0, train loss: 0.660, epsilon: 0.021\n",
            "[time:32100]test score: -94.0, train loss: 0.747, epsilon: 0.021\n",
            "[time:32200]test score: -79.0, train loss: 0.781, epsilon: 0.021\n",
            "[time:32300]test score: -99.0, train loss: 1.286, epsilon: 0.021\n",
            "[time:32400]test score: -70.0, train loss: 0.642, epsilon: 0.021\n",
            "[time:32500]test score: -94.0, train loss: 1.049, epsilon: 0.021\n",
            "[time:32600]test score: -107.0, train loss: 0.992, epsilon: 0.021\n",
            "[time:32700]test score: -500.0, train loss: 0.551, epsilon: 0.021\n",
            "[time:32800]test score: -77.0, train loss: 0.700, epsilon: 0.021\n",
            "[time:32900]test score: -109.0, train loss: 0.784, epsilon: 0.021\n",
            "[time:33000]test score: -79.0, train loss: 0.666, epsilon: 0.020\n",
            "[time:33100]test score: -89.0, train loss: 0.746, epsilon: 0.020\n",
            "[time:33200]test score: -80.0, train loss: 0.625, epsilon: 0.020\n",
            "[time:33300]test score: -84.0, train loss: 0.598, epsilon: 0.020\n",
            "[time:33400]test score: -72.0, train loss: 0.940, epsilon: 0.020\n",
            "[time:33500]test score: -107.0, train loss: 0.588, epsilon: 0.020\n",
            "[time:33600]test score: -73.0, train loss: 0.723, epsilon: 0.020\n",
            "[time:33700]test score: -70.0, train loss: 0.954, epsilon: 0.020\n",
            "[time:33800]test score: -128.0, train loss: 0.743, epsilon: 0.020\n",
            "[time:33900]test score: -74.0, train loss: 0.626, epsilon: 0.019\n",
            "[time:34000]test score: -75.0, train loss: 1.076, epsilon: 0.019\n",
            "[time:34100]test score: -102.0, train loss: 0.576, epsilon: 0.019\n",
            "[time:34200]test score: -70.0, train loss: 0.649, epsilon: 0.019\n",
            "[time:34300]test score: -69.0, train loss: 0.788, epsilon: 0.019\n",
            "[time:34400]test score: -69.0, train loss: 0.640, epsilon: 0.019\n",
            "[time:34500]test score: -99.0, train loss: 1.214, epsilon: 0.019\n",
            "[time:34600]test score: -81.0, train loss: 0.427, epsilon: 0.019\n",
            "[time:34700]test score: -97.0, train loss: 0.508, epsilon: 0.018\n",
            "[time:34800]test score: -75.0, train loss: 0.550, epsilon: 0.018\n",
            "[time:34900]test score: -71.0, train loss: 1.114, epsilon: 0.018\n",
            "[time:35000]test score: -71.0, train loss: 0.549, epsilon: 0.018\n",
            "[time:35100]test score: -72.0, train loss: 0.800, epsilon: 0.018\n",
            "[time:35200]test score: -62.0, train loss: 0.815, epsilon: 0.018\n",
            "[time:35300]test score: -71.0, train loss: 0.934, epsilon: 0.018\n",
            "[time:35400]test score: -75.0, train loss: 0.705, epsilon: 0.018\n",
            "[time:35500]test score: -87.0, train loss: 0.601, epsilon: 0.018\n",
            "[time:35600]test score: -64.0, train loss: 0.814, epsilon: 0.018\n",
            "[time:35700]test score: -74.0, train loss: 0.940, epsilon: 0.018\n",
            "[time:35800]test score: -63.0, train loss: 0.588, epsilon: 0.017\n",
            "[time:35900]test score: -77.0, train loss: 0.683, epsilon: 0.017\n",
            "[time:36000]test score: -82.0, train loss: 0.437, epsilon: 0.017\n",
            "[time:36100]test score: -69.0, train loss: 0.491, epsilon: 0.017\n",
            "[time:36200]test score: -62.0, train loss: 1.511, epsilon: 0.017\n",
            "[time:36300]test score: -73.0, train loss: 0.696, epsilon: 0.017\n",
            "[time:36400]test score: -101.0, train loss: 0.584, epsilon: 0.017\n",
            "[time:36500]test score: -91.0, train loss: 0.819, epsilon: 0.017\n",
            "[time:36600]test score: -96.0, train loss: 0.963, epsilon: 0.017\n",
            "[time:36700]test score: -90.0, train loss: 0.583, epsilon: 0.017\n",
            "[time:36800]test score: -77.0, train loss: 0.692, epsilon: 0.016\n",
            "[time:36900]test score: -61.0, train loss: 1.259, epsilon: 0.016\n",
            "[time:37000]test score: -99.0, train loss: 0.697, epsilon: 0.016\n",
            "[time:37100]test score: -69.0, train loss: 0.910, epsilon: 0.016\n",
            "[time:37200]test score: -72.0, train loss: 0.886, epsilon: 0.016\n",
            "[time:37300]test score: -70.0, train loss: 0.721, epsilon: 0.016\n",
            "[time:37400]test score: -74.0, train loss: 1.126, epsilon: 0.016\n",
            "[time:37500]test score: -86.0, train loss: 1.022, epsilon: 0.016\n",
            "[time:37600]test score: -73.0, train loss: 0.880, epsilon: 0.016\n",
            "[time:37700]test score: -69.0, train loss: 0.893, epsilon: 0.016\n",
            "[time:37800]test score: -119.0, train loss: 0.656, epsilon: 0.016\n",
            "[time:37900]test score: -91.0, train loss: 1.137, epsilon: 0.015\n",
            "[time:38000]test score: -77.0, train loss: 1.181, epsilon: 0.015\n",
            "[time:38100]test score: -74.0, train loss: 0.874, epsilon: 0.015\n",
            "[time:38200]test score: -85.0, train loss: 0.570, epsilon: 0.015\n",
            "[time:38300]test score: -62.0, train loss: 1.208, epsilon: 0.015\n",
            "[time:38400]test score: -104.0, train loss: 1.275, epsilon: 0.015\n",
            "[time:38500]test score: -89.0, train loss: 1.334, epsilon: 0.015\n",
            "[time:38600]test score: -69.0, train loss: 1.030, epsilon: 0.015\n",
            "[time:38700]test score: -75.0, train loss: 0.901, epsilon: 0.015\n",
            "[time:38800]test score: -70.0, train loss: 1.016, epsilon: 0.015\n",
            "[time:38900]test score: -74.0, train loss: 0.952, epsilon: 0.015\n",
            "[time:39000]test score: -98.0, train loss: 0.973, epsilon: 0.015\n",
            "[time:39100]test score: -77.0, train loss: 1.012, epsilon: 0.014\n",
            "[time:39200]test score: -70.0, train loss: 0.610, epsilon: 0.014\n",
            "[time:39300]test score: -70.0, train loss: 0.569, epsilon: 0.014\n",
            "[time:39400]test score: -70.0, train loss: 0.633, epsilon: 0.014\n",
            "[time:39500]test score: -500.0, train loss: 1.505, epsilon: 0.014\n",
            "[time:39600]test score: -78.0, train loss: 0.985, epsilon: 0.014\n",
            "[time:39700]test score: -70.0, train loss: 0.458, epsilon: 0.014\n",
            "[time:39800]test score: -73.0, train loss: 0.630, epsilon: 0.014\n",
            "[time:39900]test score: -79.0, train loss: 0.730, epsilon: 0.014\n",
            "[time:40000]test score: -118.0, train loss: 0.800, epsilon: 0.014\n",
            "[time:40100]test score: -76.0, train loss: 0.584, epsilon: 0.014\n",
            "[time:40200]test score: -138.0, train loss: 0.765, epsilon: 0.013\n",
            "[time:40300]test score: -84.0, train loss: 0.810, epsilon: 0.013\n",
            "[time:40400]test score: -72.0, train loss: 0.663, epsilon: 0.013\n",
            "[time:40500]test score: -69.0, train loss: 0.703, epsilon: 0.013\n",
            "[time:40600]test score: -158.0, train loss: 0.568, epsilon: 0.013\n",
            "[time:40700]test score: -79.0, train loss: 0.888, epsilon: 0.013\n",
            "[time:40800]test score: -106.0, train loss: 0.949, epsilon: 0.013\n",
            "[time:40900]test score: -71.0, train loss: 0.997, epsilon: 0.013\n",
            "[time:41000]test score: -84.0, train loss: 1.119, epsilon: 0.013\n",
            "[time:41100]test score: -89.0, train loss: 1.177, epsilon: 0.013\n",
            "[time:41200]test score: -93.0, train loss: 1.106, epsilon: 0.013\n",
            "[time:41300]test score: -62.0, train loss: 0.971, epsilon: 0.013\n",
            "[time:41400]test score: -72.0, train loss: 0.690, epsilon: 0.013\n",
            "[time:41500]test score: -62.0, train loss: 0.834, epsilon: 0.012\n",
            "[time:41600]test score: -63.0, train loss: 0.966, epsilon: 0.012\n",
            "[time:41700]test score: -80.0, train loss: 0.888, epsilon: 0.012\n",
            "[time:41800]test score: -85.0, train loss: 0.559, epsilon: 0.012\n",
            "[time:41900]test score: -165.0, train loss: 0.973, epsilon: 0.012\n",
            "[time:42000]test score: -87.0, train loss: 0.804, epsilon: 0.012\n",
            "[time:42100]test score: -76.0, train loss: 1.072, epsilon: 0.012\n",
            "[time:42200]test score: -69.0, train loss: 0.901, epsilon: 0.012\n",
            "[time:42300]test score: -88.0, train loss: 0.963, epsilon: 0.012\n",
            "[time:42400]test score: -96.0, train loss: 0.812, epsilon: 0.012\n",
            "[time:42500]test score: -63.0, train loss: 1.506, epsilon: 0.012\n",
            "[time:42600]test score: -106.0, train loss: 0.708, epsilon: 0.012\n",
            "[time:42700]test score: -93.0, train loss: 0.871, epsilon: 0.012\n",
            "[time:42800]test score: -70.0, train loss: 1.321, epsilon: 0.012\n",
            "[time:42900]test score: -70.0, train loss: 0.701, epsilon: 0.012\n",
            "[time:43000]test score: -63.0, train loss: 0.757, epsilon: 0.011\n",
            "[time:43100]test score: -92.0, train loss: 0.672, epsilon: 0.011\n",
            "[time:43200]test score: -69.0, train loss: 0.460, epsilon: 0.011\n",
            "[time:43300]test score: -84.0, train loss: 1.300, epsilon: 0.011\n",
            "[time:43400]test score: -75.0, train loss: 0.921, epsilon: 0.011\n",
            "[time:43500]test score: -63.0, train loss: 1.069, epsilon: 0.011\n",
            "[time:43600]test score: -103.0, train loss: 0.649, epsilon: 0.011\n",
            "[time:43700]test score: -500.0, train loss: 0.711, epsilon: 0.011\n",
            "[time:43800]test score: -99.0, train loss: 0.731, epsilon: 0.011\n",
            "[time:43900]test score: -71.0, train loss: 0.851, epsilon: 0.011\n",
            "[time:44000]test score: -69.0, train loss: 0.648, epsilon: 0.011\n",
            "[time:44100]test score: -71.0, train loss: 1.213, epsilon: 0.011\n",
            "[time:44200]test score: -94.0, train loss: 0.900, epsilon: 0.011\n",
            "[time:44300]test score: -500.0, train loss: 0.520, epsilon: 0.011\n",
            "[time:44400]test score: -500.0, train loss: 0.545, epsilon: 0.011\n",
            "[time:44500]test score: -69.0, train loss: 0.431, epsilon: 0.010\n",
            "[time:44600]test score: -95.0, train loss: 0.400, epsilon: 0.010\n",
            "[time:44700]test score: -63.0, train loss: 0.637, epsilon: 0.010\n",
            "[time:44800]test score: -77.0, train loss: 0.905, epsilon: 0.010\n",
            "[time:44900]test score: -94.0, train loss: 0.589, epsilon: 0.010\n",
            "[time:45000]test score: -108.0, train loss: 0.795, epsilon: 0.010\n",
            "[time:45100]test score: -80.0, train loss: 1.066, epsilon: 0.010\n",
            "[time:45200]test score: -63.0, train loss: 0.757, epsilon: 0.010\n",
            "[time:45300]test score: -89.0, train loss: 0.734, epsilon: 0.010\n",
            "[time:45400]test score: -77.0, train loss: 0.560, epsilon: 0.010\n",
            "[time:45500]test score: -90.0, train loss: 0.901, epsilon: 0.010\n",
            "[time:45600]test score: -500.0, train loss: 0.746, epsilon: 0.010\n",
            "[time:45700]test score: -500.0, train loss: 0.488, epsilon: 0.010\n",
            "[time:45800]test score: -70.0, train loss: 0.667, epsilon: 0.010\n",
            "[time:45900]test score: -106.0, train loss: 0.731, epsilon: 0.010\n",
            "[time:46000]test score: -77.0, train loss: 0.762, epsilon: 0.010\n",
            "[time:46100]test score: -107.0, train loss: 1.193, epsilon: 0.010\n",
            "[time:46200]test score: -386.0, train loss: 0.893, epsilon: 0.010\n",
            "[time:46300]test score: -500.0, train loss: 0.863, epsilon: 0.009\n",
            "[time:46400]test score: -106.0, train loss: 0.649, epsilon: 0.009\n",
            "[time:46500]test score: -86.0, train loss: 1.232, epsilon: 0.009\n",
            "[time:46600]test score: -500.0, train loss: 0.949, epsilon: 0.009\n",
            "[time:46700]test score: -74.0, train loss: 0.856, epsilon: 0.009\n",
            "[time:46800]test score: -70.0, train loss: 0.916, epsilon: 0.009\n",
            "[time:46900]test score: -130.0, train loss: 1.247, epsilon: 0.009\n",
            "[time:47000]test score: -86.0, train loss: 0.698, epsilon: 0.009\n",
            "[time:47100]test score: -87.0, train loss: 0.849, epsilon: 0.009\n",
            "[time:47200]test score: -69.0, train loss: 1.499, epsilon: 0.009\n",
            "[time:47300]test score: -94.0, train loss: 0.795, epsilon: 0.009\n",
            "[time:47400]test score: -78.0, train loss: 0.660, epsilon: 0.009\n",
            "[time:47500]test score: -85.0, train loss: 1.141, epsilon: 0.009\n",
            "[time:47600]test score: -83.0, train loss: 1.851, epsilon: 0.009\n",
            "[time:47700]test score: -86.0, train loss: 0.768, epsilon: 0.009\n",
            "[time:47800]test score: -75.0, train loss: 0.922, epsilon: 0.009\n",
            "[time:47900]test score: -91.0, train loss: 0.849, epsilon: 0.009\n",
            "[time:48000]test score: -74.0, train loss: 0.643, epsilon: 0.009\n",
            "[time:48100]test score: -106.0, train loss: 0.777, epsilon: 0.009\n",
            "[time:48200]test score: -500.0, train loss: 0.982, epsilon: 0.009\n",
            "[time:48300]test score: -86.0, train loss: 1.317, epsilon: 0.009\n",
            "[time:48400]test score: -94.0, train loss: 1.658, epsilon: 0.009\n",
            "[time:48500]test score: -85.0, train loss: 1.372, epsilon: 0.009\n",
            "[time:48600]test score: -90.0, train loss: 0.726, epsilon: 0.008\n",
            "[time:48700]test score: -85.0, train loss: 0.686, epsilon: 0.008\n",
            "[time:48800]test score: -68.0, train loss: 0.933, epsilon: 0.008\n",
            "[time:48900]test score: -62.0, train loss: 0.684, epsilon: 0.008\n",
            "[time:49000]test score: -61.0, train loss: 1.422, epsilon: 0.008\n",
            "[time:49100]test score: -83.0, train loss: 1.555, epsilon: 0.008\n",
            "[time:49200]test score: -73.0, train loss: 0.956, epsilon: 0.008\n",
            "[time:49300]test score: -61.0, train loss: 0.843, epsilon: 0.008\n",
            "[time:49400]test score: -62.0, train loss: 0.850, epsilon: 0.008\n",
            "[time:49500]test score: -63.0, train loss: 1.212, epsilon: 0.008\n",
            "[time:49600]test score: -88.0, train loss: 0.935, epsilon: 0.008\n",
            "[time:49700]test score: -83.0, train loss: 0.649, epsilon: 0.008\n",
            "[time:49800]test score: -77.0, train loss: 0.996, epsilon: 0.008\n",
            "[time:49900]test score: -70.0, train loss: 1.188, epsilon: 0.008\n",
            "[time:50000]test score: -71.0, train loss: 1.096, epsilon: 0.008\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import gym\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = dict(\n",
        "    env_name            = \"Acrobot-v1\", # **환경은 변경 금지**\n",
        "    batch               = 128,          # 배치 크기\n",
        "    buffer_size         = 10000,        # 버퍼의 최대 크기(transition 단위)\n",
        "    lr                  = 0.001,        # 학습률\n",
        "    gamma               = 0.99,         # discount(할인율)\n",
        "    epsilon_init        = 0.1,          # 초기 epsilon(=초기 탐험 비율)\n",
        "    epsilon_min         = 0.001,        # 최소 epsilon(=최종 탐험 비율)\n",
        "    epsilon_decay       = 0.995,        # epsilon 감소 비율(매 episode마다 epsilon에 곱해짐)\n",
        "    n_step              = 50000,        # 학습 횟수 (time step 단위)\n",
        "    n_train_start       = 8000,         # 초기 버퍼 채우기(time step 단위)\n",
        "    target_update_freq  = 100,          # target Q의 업데이트 주기(time step 단위)\n",
        "    test_freq           = 100,          # test 주기(time step 단위)\n",
        ")\n",
        "\n",
        "env      = gym.make(config[\"env_name\"])\n",
        "test_env = gym.make(config[\"env_name\"])\n",
        "dState = env.observation_space.shape[0]\n",
        "dAction = env.action_space.n\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self, dState, dAction):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            # --- Q1: 입력층 ---\n",
        "            nn.ReLU(),\n",
        "            # --- Q2: 은닉층 ---\n",
        "            nn.ReLU(),\n",
        "            # --- Q3: 출력층 ---\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "Q = Qnet(dState=dState, dAction=dAction).to(DEVICE)\n",
        "targetQ = Qnet(dState=dState, dAction=dAction).to(DEVICE)\n",
        "# --- Q4: target Q 네트워크를 Q 로 업데이트 ---\n",
        "optimizerQ = optim.Adam(Q.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "replay_buffer = deque(maxlen=config[\"buffer_size\"])\n",
        "\n",
        "gamma = config[\"gamma\"]\n",
        "epsilon = config[\"epsilon_init\"]\n",
        "\n",
        "def getAction(state, dAction, epsilon, Q):\n",
        "    with torch.no_grad():\n",
        "        if (random.random() > epsilon):\n",
        "            state = torch.from_numpy(state).float().to(DEVICE)\n",
        "            # --- Q5: state에서 모든 Q를 계산하고 가장 큰 행동 선택 ---\n",
        "        else:\n",
        "            # --- Q6: 랜덤 action 선택 ---\n",
        "    return action\n",
        "\n",
        "def test(env, Q, dAction, epsilon):\n",
        "    with torch.no_grad():\n",
        "        score = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = getAction(state, dAction, epsilon, Q)\n",
        "            nextState, reward, done, _info = env.step(action)\n",
        "            score += reward\n",
        "            state = nextState\n",
        "            if done:\n",
        "                break\n",
        "    return score\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "for t in range(1, config[\"n_train_start\"]+1):\n",
        "    action = getAction(state, dAction, epsilon, Q)\n",
        "    nextState, reward, done, _info = env.step(action)\n",
        "    transition = (state, action, reward, nextState, done)\n",
        "    replay_buffer.append(transition)\n",
        "    state = nextState\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "\n",
        "score = 0.0\n",
        "scores = []\n",
        "test_scores = []\n",
        "losses = []\n",
        "qs = []\n",
        "\n",
        "state = env.reset()\n",
        "for t in range(1, config[\"n_step\"]+1):\n",
        "    action = getAction(state, dAction, epsilon, Q)\n",
        "    nextState, reward, done, _info = env.step(action)\n",
        "    transition = (state, action, reward, nextState, done)\n",
        "    replay_buffer.append(transition)\n",
        "\n",
        "    score += reward\n",
        "    state = nextState\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        scores.append(score)\n",
        "        score = 0.0\n",
        "        epsilon = max(config[\"epsilon_min\"], epsilon*config[\"epsilon_decay\"])\n",
        "\n",
        "    transitions = random.sample(replay_buffer, config[\"batch\"])\n",
        "    batch = []\n",
        "    for item in zip(*transitions):\n",
        "        item = torch.from_numpy(np.stack(item)).float().to(DEVICE)\n",
        "        batch.append(item)\n",
        "    states, actions, rewards, nextStates, dones = batch\n",
        "    actions = actions.unsqueeze(dim=-1).long()\n",
        "    rewards = rewards.unsqueeze(dim=-1)\n",
        "    dones = dones.unsqueeze(dim=-1)\n",
        "\n",
        "    # --- Q7: estimateQs 계산 ---\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # --- Q8: nextTargetQs 계산 ---\n",
        "        # --- Q9: targetQs 게산 ---\n",
        "\n",
        "    # --- Q10: 목적함수 MSE loss 계산 ---\n",
        "\n",
        "    optimizerQ.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizerQ.step()\n",
        "    losses.append(loss.detach().cpu().item())\n",
        "    qs.append(estimateQs.detach().mean().cpu().item())\n",
        "\n",
        "    if t % config[\"target_update_freq\"] == 0:\n",
        "        targetQ.load_state_dict(Q.state_dict())\n",
        "\n",
        "    if t % config[\"test_freq\"] == 0:\n",
        "        test_score = test(test_env, Q, dAction, epsilon=config[\"epsilon_min\"])\n",
        "        test_scores.append(test_score)\n",
        "        print(f'[time:{t}]test score: {test_score}, train loss: {losses[-1]:.3f}, epsilon: {epsilon:.3f}')\n",
        "\n",
        "\n",
        "print(f'최종 점수: {np.array(test_scores[-10:]).mean()}')\n",
        "with open(\"result.csv\", \"w\") as f:\n",
        "    f.write(str(np.array(test_scores[-10:]).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzePef6StyS4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.system(\"elice_grade result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnvAGnvit2Tf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}