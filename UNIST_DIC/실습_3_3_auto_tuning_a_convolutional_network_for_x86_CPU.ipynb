{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongheechoi/education/blob/main/%EC%8B%A4%EC%8A%B5_3_3_auto_tuning_a_convolutional_network_for_x86_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20QUz_c8_9J-"
      },
      "source": [
        "# **TVM 실습자료 3.3: Auto-tuning a Convolutional Network for x86 CPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3gTKE4Lq_8Ud"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "!pip list | grep numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjmNkWPO2kKS",
        "outputId": "85c325da-0c9b-4f32-f39c-a0077b8ca6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Collecting apache-tvm\n",
            "  Downloading apache_tvm-0.14.dev273-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (25.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (1.15.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (6.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from apache-tvm) (4.14.1)\n",
            "Downloading apache_tvm-0.14.dev273-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: apache-tvm\n",
            "Successfully installed apache-tvm-0.14.dev273\n"
          ]
        }
      ],
      "source": [
        "# Linux/MacOS CPU build only!\n",
        "# See tlcpack.ai for other pre-built binaries including CUDA\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install apache-tvm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6_w6Ypq9Eg6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "from tvm import relay, autotvm\n",
        "from tvm.relay import testing\n",
        "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
        "from tvm.autotvm.graph_tuner import DPTuner, PBQPTuner\n",
        "import tvm.contrib.graph_executor as runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeC_d6rG_wca"
      },
      "source": [
        "**Define network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjVatTpN_xu6"
      },
      "outputs": [],
      "source": [
        "def get_network(name, batch_size):\n",
        "    \"\"\"Get the symbol definition and random weight of a network\"\"\"\n",
        "    input_shape = (batch_size, 3, 224, 224)\n",
        "    output_shape = (batch_size, 1000)\n",
        "\n",
        "    if \"resnet\" in name:\n",
        "        n_layer = int(name.split(\"-\")[1])\n",
        "        mod, params = relay.testing.resnet.get_workload(\n",
        "            num_layers=n_layer, batch_size=batch_size, dtype=dtype\n",
        "        )\n",
        "    elif \"vgg\" in name:\n",
        "        n_layer = int(name.split(\"-\")[1])\n",
        "        mod, params = relay.testing.vgg.get_workload(\n",
        "            num_layers=n_layer, batch_size=batch_size, dtype=dtype\n",
        "        )\n",
        "    elif name == \"mobilenet\":\n",
        "        mod, params = relay.testing.mobilenet.get_workload(batch_size=batch_size, dtype=dtype)\n",
        "    elif name == \"squeezenet_v1.1\":\n",
        "        mod, params = relay.testing.squeezenet.get_workload(\n",
        "            batch_size=batch_size, version=\"1.1\", dtype=dtype\n",
        "        )\n",
        "    elif name == \"inception_v3\":\n",
        "        input_shape = (batch_size, 3, 299, 299)\n",
        "        mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported network: \" + name)\n",
        "\n",
        "    return mod, params, input_shape, output_shape\n",
        "\n",
        "\n",
        "# Replace \"llvm\" with the correct target of your CPU.\n",
        "# For example, for AWS EC2 c5 instance with Intel Xeon\n",
        "# Platinum 8000 series, the target should be \"llvm -mcpu=skylake-avx512\".\n",
        "# For AWS EC2 c4 instance with Intel Xeon E5-2666 v3, it should be\n",
        "# \"llvm -mcpu=core-avx2\".\n",
        "target = \"llvm\"\n",
        "\n",
        "batch_size = 1\n",
        "dtype = \"float32\"\n",
        "model_name = \"resnet-18\"\n",
        "log_file = \"%s.log\" % model_name\n",
        "graph_opt_sch_file = \"%s_graph_opt.log\" % model_name\n",
        "\n",
        "# Set the input name of the graph\n",
        "# For ONNX models, it is typically \"0\".\n",
        "input_name = \"data\"\n",
        "\n",
        "# Set number of threads used for tuning based on the number of\n",
        "# physical CPU cores on your machine.\n",
        "num_threads = 2\n",
        "os.environ[\"TVM_NUM_THREADS\"] = str(num_threads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eqGRigBOFQ"
      },
      "source": [
        "**Configure tensor tuning settings and create tasks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUrty_w8BL6n"
      },
      "outputs": [],
      "source": [
        "tuning_option = {\n",
        "    \"log_filename\": log_file,\n",
        "    \"tuner\": \"random\",\n",
        "    \"early_stopping\": None,\n",
        "    \"measure_option\": autotvm.measure_option(\n",
        "        builder=autotvm.LocalBuilder(),\n",
        "        runner=autotvm.LocalRunner(\n",
        "            number=1, repeat=10, min_repeat_ms=0, enable_cpu_cache_flush=True\n",
        "        ),\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "# You can skip the implementation of this function for this tutorial.\n",
        "def tune_kernels(\n",
        "    tasks, measure_option, tuner=\"gridsearch\", early_stopping=None, log_filename=\"tuning.log\"\n",
        "):\n",
        "\n",
        "    for i, task in enumerate(tasks):\n",
        "        prefix = \"[Task %2d/%2d] \" % (i + 1, len(tasks))\n",
        "\n",
        "        # create tuner\n",
        "        if tuner == \"xgb\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"reg\")\n",
        "        elif tuner == \"xgb_knob\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"knob\")\n",
        "        elif tuner == \"xgb_itervar\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"itervar\")\n",
        "        elif tuner == \"xgb_curve\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"curve\")\n",
        "        elif tuner == \"xgb_rank\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank\")\n",
        "        elif tuner == \"xgb_rank_knob\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"knob\")\n",
        "        elif tuner == \"xgb_rank_itervar\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"itervar\")\n",
        "        elif tuner == \"xgb_rank_curve\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"curve\")\n",
        "        elif tuner == \"xgb_rank_binary\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank-binary\")\n",
        "        elif tuner == \"xgb_rank_binary_knob\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"knob\")\n",
        "        elif tuner == \"xgb_rank_binary_itervar\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"itervar\")\n",
        "        elif tuner == \"xgb_rank_binary_curve\":\n",
        "            tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"curve\")\n",
        "        elif tuner == \"ga\":\n",
        "            tuner_obj = GATuner(task, pop_size=50)\n",
        "        elif tuner == \"random\":\n",
        "            tuner_obj = RandomTuner(task)\n",
        "        elif tuner == \"gridsearch\":\n",
        "            tuner_obj = GridSearchTuner(task)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid tuner: \" + tuner)\n",
        "\n",
        "        # do tuning\n",
        "        n_trial = len(task.config_space)\n",
        "        tuner_obj.tune(\n",
        "            n_trial=n_trial,\n",
        "            early_stopping=early_stopping,\n",
        "            measure_option=measure_option,\n",
        "            callbacks=[\n",
        "                autotvm.callback.progress_bar(n_trial, prefix=prefix),\n",
        "                autotvm.callback.log_to_file(log_filename),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "\n",
        "# Use graph tuner to achieve graph level optimal schedules\n",
        "# Set use_DP=False if it takes too long to finish.\n",
        "def tune_graph(graph, dshape, records, opt_sch_file, use_DP=True):\n",
        "    target_op = [\n",
        "        relay.op.get(\"nn.conv2d\"),\n",
        "    ]\n",
        "    Tuner = DPTuner if use_DP else PBQPTuner\n",
        "    executor = Tuner(graph, {input_name: dshape}, records, target_op, target)\n",
        "    executor.benchmark_layout_transform(min_exec_num=2000)\n",
        "    executor.run()\n",
        "    executor.write_opt_sch2record_file(opt_sch_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7rv60sjDlSk",
        "outputId": "798379ec-92ee-4e3d-fd52-93f9ad923ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract tasks...\n",
            "[Task  2/12]  Current/Best:   18.09/  33.16 GFLOPS | Progress: (980/980) | 767.08 s Done.\n",
            "[Task  3/12]  Current/Best:   25.27/  31.49 GFLOPS | Progress: (980/980) | 657.04 s Done.\n",
            "[Task  4/12]  Current/Best:    6.25/  33.07 GFLOPS | Progress: (896/896) | 609.36 s Done.\n",
            "[Task  5/12]  Current/Best:   17.51/  29.26 GFLOPS | Progress: (896/896) | 595.49 s Done.\n",
            "[Task  6/12]  Current/Best:   11.89/  31.19 GFLOPS | Progress: (1024/1024) | 868.45 s Done.\n",
            "[Task  7/12]  Current/Best:   18.65/  32.62 GFLOPS | Progress: (864/864) | 684.35 s Done.\n",
            "[Task  8/12]  Current/Best:    7.05/  29.90 GFLOPS | Progress: (864/864) | 577.35 s Done.\n",
            "[Task  9/12]  Current/Best:    6.01/  29.89 GFLOPS | Progress: (972/972) | 868.28 s Done.\n",
            "[Task 10/12]  Current/Best:   22.43/  23.33 GFLOPS | Progress: (8/720) | 9.90 s Done.\n",
            "[Task 10/12]  Current/Best:   19.52/  27.94 GFLOPS | Progress: (720/720) | 682.21 s Done.\n",
            "[Task 11/12]  Current/Best:   17.84/  30.32 GFLOPS | Progress: (720/720) | 478.44 s Done.\n",
            "[Task 12/12]  Current/Best:   21.08/  28.25 GFLOPS | Progress: (800/800) | 867.99 s Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 07:56:18,909 INFO Start to benchmark layout transformation...\n",
            "2025-07-11 08:13:10,451 INFO Benchmarking layout transformation successful.\n",
            "2025-07-11 08:13:10,505 INFO Start to run dynamic programming algorithm...\n",
            "2025-07-11 08:13:10,506 INFO Start forward pass...\n",
            "2025-07-11 08:13:10,708 INFO Finished forward pass.\n",
            "2025-07-11 08:13:10,709 INFO Start backward pass...\n",
            "2025-07-11 08:13:10,764 INFO Finished backward pass...\n",
            "2025-07-11 08:13:10,765 INFO Finished DPExecutor run.\n",
            "2025-07-11 08:13:10,769 INFO Writing optimal schedules to resnet-18_graph_opt.log successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation of the network compiled in 'default' mode without auto tune:\n",
            "Compile...\n",
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  168.6298     166.4804     174.3390     165.0700      4.0778                  \n",
            "\n",
            "Evaluation of the network been tuned on kernel level:\n",
            "Compile...\n",
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  137.6327     137.3943     138.2141     137.2896      0.4134                  \n",
            "\n",
            "Evaluation of the network been tuned on graph level:\n",
            "Compile...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:autotvm:Config for target=llvm -keys=cpu , workload=('dense_nopack.x86', ('TENSOR', (1, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32') is missing in ApplyGraphBest context. A fallback configuration is used, which may bring great performance regression.\n",
            "WARNING:autotvm:Config for target=llvm -keys=cpu , workload=('dense_pack.x86', ('TENSOR', (1, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32') is missing in ApplyGraphBest context. A fallback configuration is used, which may bring great performance regression.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluate inference time cost...\n",
            "Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  140.2728     137.9159     146.3428     136.5596      4.3277                  \n"
          ]
        }
      ],
      "source": [
        "def evaluate_performance(lib, data_shape):\n",
        "    # upload parameters to device\n",
        "    dev = tvm.cpu()\n",
        "    data_tvm = tvm.nd.array((np.random.uniform(size=data_shape)).astype(dtype))\n",
        "    module = runtime.GraphModule(lib[\"default\"](dev))\n",
        "    module.set_input(input_name, data_tvm)\n",
        "\n",
        "    # evaluate\n",
        "    print(\"Evaluate inference time cost...\")\n",
        "    print(module.benchmark(dev, number=100, repeat=3))\n",
        "\n",
        "\n",
        "def tune_and_evaluate(tuning_opt):\n",
        "    # extract workloads from relay program\n",
        "    print(\"Extract tasks...\")\n",
        "    mod, params, data_shape, out_shape = get_network(model_name, batch_size)\n",
        "    tasks = autotvm.task.extract_from_program(\n",
        "        mod[\"main\"], target=target, params=params, ops=(relay.op.get(\"nn.conv2d\"),)\n",
        "    )\n",
        "\n",
        "    # run tuning tasks\n",
        "    tune_kernels(tasks, **tuning_opt)\n",
        "    tune_graph(mod[\"main\"], data_shape, log_file, graph_opt_sch_file)\n",
        "\n",
        "    # compile kernels in default mode\n",
        "    print(\"Evaluation of the network compiled in 'default' mode without auto tune:\")\n",
        "    with tvm.transform.PassContext(opt_level=3):\n",
        "        print(\"Compile...\")\n",
        "        lib = relay.build(mod, target=target, params=params)\n",
        "        evaluate_performance(lib, data_shape)\n",
        "\n",
        "    # compile kernels in kernel tuned only mode\n",
        "    print(\"\\nEvaluation of the network been tuned on kernel level:\")\n",
        "    with autotvm.apply_history_best(log_file):\n",
        "        print(\"Compile...\")\n",
        "        with tvm.transform.PassContext(opt_level=3):\n",
        "            lib = relay.build(mod, target=target, params=params)\n",
        "        evaluate_performance(lib, data_shape)\n",
        "\n",
        "    # compile kernels with graph-level best records\n",
        "    print(\"\\nEvaluation of the network been tuned on graph level:\")\n",
        "    with autotvm.apply_graph_best(graph_opt_sch_file):\n",
        "        print(\"Compile...\")\n",
        "        with tvm.transform.PassContext(opt_level=3):\n",
        "            lib = relay.build_module.build(mod, target=target, params=params)\n",
        "        evaluate_performance(lib, data_shape)\n",
        "\n",
        "\n",
        "# We do not run the tuning in our webpage server since it takes too long.\n",
        "# Uncomment the following line to run it by yourself.\n",
        "\n",
        "tune_and_evaluate(tuning_option)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
